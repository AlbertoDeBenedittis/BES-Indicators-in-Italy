{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BES INDICATORS IN ITALY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [__Bes project__](https://www4.istat.it/en/well-being-and-sustainability/well-being-measures/bes-report) \n",
    "was launched in 2010 to measure Equitable and Sustainable Well-being,\n",
    "and with the aim of evaluating the progress of society not only from an economic,\n",
    "but also from a social and environmental point of view. \n",
    "\n",
    "\n",
    "To this aim, the traditional economic indicators, GDP first of all, have been integrated with measures of the quality of people’s life and of the environment.\n",
    "\n",
    "\n",
    "Since 2016, well-being indicators and welfare analyzers have been presented with indicators for monitoring the objectives of the 2030 Agenda\n",
    "for Sustainable Development,\n",
    "the so-called [Sustainable Development Goals (SDGs)](https://sdgs.un.org/goals) of the United Nations.\n",
    "They were chosen by the global community through a political agreement between the different actors, to represent their values, priorities and objectives.\n",
    "The United Nations Statistical Commission (UNSC) has set up a shared set of statistical information to monitor the progress of individual countries towards the SDGs,\n",
    "including over two hundred indicators.\n",
    "The two sets of indicators are only partially overlapping, but certainly complementary.\n",
    "\n",
    "\n",
    "Bes' indicators cover [12 domains relevant for the measuramente of the well-being](https://www.istat.it/it/files/2018/04/12-domains-scientific-commission.pdf) \n",
    "and they are the following: \n",
    "\n",
    "1)  Health \n",
    "2)  Education & Training\n",
    "3)  Work & Life Balance\n",
    "4)  Economic well-being\n",
    "5)  Social Relationships\n",
    "6)  Politics & Istitutions\n",
    "7)  Security \n",
    "8)  Subjective well-being\n",
    "9)  Landscape & Cultural heritage\n",
    "10) Environment\n",
    "11) Innovation, Research & Creativity\n",
    "12) Quality of services\n",
    "\n",
    "These twleve indicators are themselves subdivided into smaller subindicators and each of them refer to a specific measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dowloaded from the national institute of statistics is organized in a huge excel file. This file contains the data for all the indicators. \n",
    "<br>\n",
    "In order to analyze and represent the data there is the need to split this file in smaller files: One for each subindicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dependencies\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "dati_bes = pd.read_excel('D:\\Geospatial_Project\\Dati Bes\\Bes\\Indicatori_per_provincia_sesso.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "header = dati_bes.columns\n",
    "tabella_new = pd.DataFrame(columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data in an automatic manner there is the need to create an algorithm that detects whether there is a change in the unit of measure or in the indicator.\n",
    "<br>\n",
    "A smart way to do so is by looking at the territories. Indeed, there is a change in the subindicators everytime the data refer to ´Italy´ which is the average that for that specific measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern that change \n",
    "for j in range(len(dati_bes)):\n",
    "    mini_ds = dati_bes.iloc[j]\n",
    "    tabella_new = tabella_new.append(mini_ds)\n",
    "    if mini_ds['TERRITORIO'] == 'Italia':\n",
    "        Dominio = dati_bes.iloc[j]['DOMINIO']\n",
    "        Indicatore = dati_bes.iloc[j]['INDICATORE']\n",
    "        Sesso = dati_bes.iloc[j]['SESSO']\n",
    "        Unita = dati_bes.iloc[j]['UNITA_MISURA']\n",
    "        tabella_new.to_excel('D:/Geospatial_Project/Nuovi_Dati2/{Dominio}-{Indicatore}-{Sesso}-{Unita}.xlsx'.format(Dominio=Dominio, Indicatore=Indicatore, Sesso = Sesso, Unita = Unita))\n",
    "        tabella_new = pd.DataFrame(columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the data has been properly stored\n",
    "files_ = os.listdir(path='Nuovi_Dati2')\n",
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether all the datasets have the same lengths. If not there is the need to \n",
    "#understand why\n",
    "to_chekc = []\n",
    "for file in files_:\n",
    "    if file.endswith('.xlsx'):\n",
    "        excel_ = pd.read_excel('Nuovi_Dati2/'+file)\n",
    "        if len(excel_) != 135:\n",
    "            print(file + str(len(excel_)))\n",
    "            to_chekc.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some datasets whose lengths is sligthly different compared to others. \n",
    "<br>\n",
    "Some hypothesis can be made in order to justify these differences:\n",
    "* maybe the province does not send the required data and so it has not been considered;\n",
    "* the province did not exist when the data collection started (the reference year is 2004) \n",
    "<br>\n",
    "\n",
    "To understand the real cause there is the need to see which territories are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prova = pd.read_excel('Nuovi_Dati2/Benessere economico-Retribuzione media annua dei lavoratori dipendenti-Totale-euro.xlsx')\n",
    "s2 = df_prova.TERRITORIO.unique().tolist()\n",
    "path = 'Nuovi_Dati2/'\n",
    "Territori_Mancanti = {}\n",
    "for f in to_chekc:\n",
    "    df_check = pd.read_excel(path + f)\n",
    "    s1 =df_check.TERRITORIO.unique().tolist()\n",
    "    mis_val = set(s2) - set(s1) \n",
    "    Territori_Mancanti[f] = mis_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territori_Mancanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell, it is possible to notice that all the missing territories refer to the region of Sardinia. Hence it is possible to believe that the 'missing territories' refer to territories that have changed their territorial administration or something like that.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has been stored in smaller files, one for each subindicators, it is possible to start the analysis and the representation of the data. \n",
    "<br>\n",
    "Firstly, there is the need to decide how do we want to organize the analysis: Indeed, we have data of related to provinces, regions and macroareas (north, centre, south) all together. Thus, there is the need to split the data in three subsets, one for each level of analysis:\n",
    "*  Macrolevel : A comparison between North, Center, and South of Italy:\n",
    "    * North = Piemonte, Valle d'Aosta, Lombardia, Liguria, Trentino-Alto Adige, Veneto, Friuli-Venezia-Giulia, Emilia-Romagna\n",
    "    * Center: Toscana, Umbria, Marche, Lazio\n",
    "    * South: Abruzzo, Molise, Campania, Puglia, Basilicata, Calabria, Sicilia, Sardegna\n",
    "* Regional level: A comparison among the twenty italian regions\n",
    "* Province level : A comperison among the one hundred and seven italian provinces. \n",
    "<br>\n",
    "\n",
    "\n",
    "Since we will end up with many differnt datasets I choose to create functions that clean and manipulate the datasets in an automatic manner and according to the diffent needs. \n",
    "<br>\n",
    "To clarify we will start having ordinary pandas dataframes and the output will be a geopandas dataframe that will allow us to create geospatial representation and to perform some kind of data analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required dependencies\n",
    "import os \n",
    "import esda\n",
    "import math\n",
    "import folium\n",
    "import leafmap\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "import libpysal as lps\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dati_bes(path):\n",
    "    \n",
    "    '''\n",
    "    Through this function the excel file:\n",
    "    * is read and imported;\n",
    "    * is reduced by dropping unuseful columns     \n",
    "    '''\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df.drop(columns = ['Unnamed: 0', 'DOMINIO', 'CODICE', 'SESSO', 'FONTE'], inplace = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "def provinces_BES(df, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions):\n",
    "\n",
    "    '''\n",
    "    Due to the difference in annotation between the BES_Indicator data and the one of ISTAT,\n",
    "    there is the need to change some of the names of the provinces.\n",
    "    Firstly, we should brake the dataset of BES Indicators in order to separate regions and macro areas,\n",
    "    NB: We have to check that there are NO differences between regions'names\n",
    "    Secondly, we have to omologate the names of the two dataset.\n",
    "    NB: There are substantial differences mainly involving Sardinia, where ISTAT's Sud-Sardegnia contains BES'S Sud-Sardegna, Medio-Campidano, Carbonia-Iglesias, Ogliastra and Olbia-Tempio.\n",
    "    '''\n",
    "\n",
    "    #all_territories = df.TERRITORIO.unique().tolist()\n",
    "    #return set(all_territories) - set(Regions) - set(Anomalous_Regions) - set(Sud_Sardinia) - set(Macro_Areas) - set(Italy)\n",
    "    all_territories = df.TERRITORIO.unique().tolist()\n",
    "    territories =  set(all_territories) - set(Regions) - set(Anomalous_Regions) - set(Sud_Sardinia) - set(Macro_Areas) - set(Italy)\n",
    "    territories = list(territories)\n",
    "    territories.sort()\n",
    "    return territories\n",
    "\n",
    "def regions_BES(Regions):\n",
    "    '''\n",
    "    Due to the difference in annotation between the BES_Indicator data and the ones of ISTAT,\n",
    "    there is the need to change some of the names of the regions.\n",
    "    '''\n",
    "\n",
    "    new_regions = []\n",
    "    for reg in Regions:\n",
    "        if reg ==\"Valle d'Aosta\":\n",
    "            new_regions.append(\"Valle d'Aosta/Vallée d'Aoste\")\n",
    "        elif reg == 'Trentino-Alto Adige':\n",
    "            new_regions.append(\"Trentino-Alto Adige/Südtirol\")\n",
    "        elif reg == 'Friuli Venezia Giulia':\n",
    "            new_regions.append('Friuli-Venezia Giulia')\n",
    "        else:\n",
    "            new_regions.append(reg)\n",
    "    \n",
    "    return new_regions\n",
    "\n",
    "\n",
    "def mod_col_geo(geodf):\n",
    "    \n",
    "    '''\n",
    "    This function drops useless columns to the geodataframe and rename one of its column\n",
    "    '''\n",
    "    \n",
    "    geodf.drop(columns=['COD_RIP','COD_REG'], inplace = True)\n",
    "    geodf.rename(columns={'DEN_REG':'Reg'}, inplace = True) \n",
    "    \n",
    "    return geodf\n",
    "\n",
    "def get_regions_names(geodf):  \n",
    "\n",
    "    '''\n",
    "    This function returns a list of all the regions names according to the ISTAT standard\n",
    "    NB there are some differences among the names used by ISTAT and the ones used for the BES indicators \n",
    "    '''\n",
    "\n",
    "    return  geodf.Reg.unique().tolist() \n",
    "\n",
    "def sort_reset_index_geo(geo_df):\n",
    "\n",
    "    '''\n",
    "    The geodaframe is sorted and its indeces are reset\n",
    "    (This will be useful when the main dataframe will be converted into a geopandas dataframe)\n",
    "    '''\n",
    "    geo_df.sort_values(by = ['Prov'], inplace = True) \n",
    "    geo_df.reset_index(inplace = True)\n",
    "\n",
    "    return geo_df\n",
    "\n",
    "def order_df(df, BES_Territories):\n",
    "\n",
    "    '''\n",
    "    The daframe is:\n",
    "    * reduced in order to have coherence with the geodatafrmae which considers less provinces than the one of bes (actually, the geodatraframe respects the actual division of Italy into provinces)\n",
    "    * sorted\n",
    "    * its indeces are reset\n",
    "    (This will be useful when the main dataframe will be converted into a geopandas dataframe)\n",
    "    '''\n",
    "\n",
    "    prov_df =  df[df['TERRITORIO'].isin(BES_Territories)]\n",
    "\n",
    "    if len(prov_df) > 3:\n",
    "        prov_df.sort_values(by=['TERRITORIO'],inplace = True)\n",
    "        \n",
    "    \n",
    "    prov_df.set_index('TERRITORIO', inplace = True)\n",
    "\n",
    "    return prov_df\n",
    "\n",
    "\n",
    "def clean_prov_geo(geo_prov, provinces):\n",
    "\n",
    "    '''\n",
    "    This functions cleans the geodataframe relative to provinces. \n",
    "    What this function does is:\n",
    "    * dropping useless columns\n",
    "    * sorting values according to territories' names\n",
    "    * setting the territories names as indexes\n",
    "    '''\n",
    "      \n",
    "    geo_prov.drop(columns = ['COD_RIP','COD_REG',\t'COD_PROV',\t'COD_CM',\t'COD_UTS',\t'DEN_PROV',\t'DEN_CM', 'TIPO_UTS'])\n",
    "    geo_prov.sort_values(by = 'DEN_UTS', inplace = True)\n",
    "    geo_prov.DEN_UTS = provinces\n",
    "    geo_prov.set_index('DEN_UTS', inplace = True)\n",
    "  \n",
    "    return geo_prov\n",
    "\n",
    "def order_df_regions(geodf, BES_Territories):\n",
    "    '''\n",
    "    This function modifies the geodaframe in order to have the same nomenclature for both the df and the geodf.\n",
    "    This step  is needed in order to merge the two at the next step.\n",
    "    '''\n",
    "    \n",
    "    geodf['Reg'] = BES_Territories #CHANGE\n",
    "    geodf.sort_values(by= ['Reg'], inplace = True)\n",
    "    geodf.set_index('Reg', inplace = True )\n",
    "    \n",
    "    return geodf\n",
    "\n",
    "def aggregate_macros(geodf):\n",
    "    \n",
    "    '''\n",
    "    The data regarding BES indicators refer to just three macroareas insted of 6 as in the ISTAT dataset. \n",
    "\n",
    "    Hence, there is the need to marge the areas in order have 3 main areas\n",
    "\n",
    "    1.   North : Piemonte, Valle d'Aosta/Vallée d'Aoste, Lombardia, Liguria, Trentino-Alto Adige/Südtirol, Veneto, Friuli-Venezia Giulia, Emilia-Romagna\n",
    "    2.   Centre: Toscana, Umbria, Marche, Lazio\n",
    "    3.   South(& Islands): Abruzzo, Molise, Campania, Puglia, Basilicata, Calabria, Sicilia, Sardegna  \n",
    "    '''\n",
    "\n",
    "    Territorio = ['Nord', 'Nord', 'Centro', 'Mezzogiorno', 'Mezzogiorno']\n",
    "    geodf['TERRITORIO'] = Territorio\n",
    "    geodf = geodf.to_crs(epsg=4326).dissolve(by='TERRITORIO')\n",
    "    geodf.drop(columns = ['DEN_RIP'])\n",
    "    geodf.sort_values(by = ['COD_RIP'], inplace = True)\n",
    "\n",
    "    return geodf\n",
    "\n",
    "def from_df_to_gdf(df, geo_df):\n",
    "\n",
    "    '''\n",
    "    With this function we convert the dataframe containing the statistics we are interested in into a geodaframe. \n",
    "    This allows us to use all the functionalities of a geopandas dataframe such us doing plots.\n",
    "    '''\n",
    "\n",
    "    df['Shape_Leng'] = geo_df['Shape_Leng']\n",
    "    df['Shape_Area'] = geo_df['Shape_Area']\n",
    "    df['geometry'] = geo_df['geometry']\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def look_for_anomalies2(df,var):\n",
    "\n",
    "    i = 0\n",
    "    in_set = set()\n",
    "\n",
    "    for el in df[var].values:\n",
    "        if type(el) == str or math.isnan(el): \n",
    "            \n",
    "            in_set.add(i)\n",
    "    \n",
    "        i += 1 \n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(list(in_set), inplace = True)   \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the geodataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, we have data regarding the different subindicators. However, in order to perform geospatial analyses and create geospatial representations there is the need to have the data about the ´geometries´ of the territories we are dealing with. \n",
    "<br>\n",
    "Thus, the first thing to do is importing the geodataframes provided by the national institute of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where are stored the shape files\n",
    "Reg_Path = 'D:/Geospatial_Project/Dati/Limiti/Lim/Reg01012021/Reg01012021_WGS84.shp'\n",
    "Prov_Path = 'D:/Geospatial_Project/Dati/Limiti/Lim/ProvCM01012021/ProvCM01012021_WGS84.shp'\n",
    "Macro_Path = 'D:/Geospatial_Project/Dati/Limiti/Lim/RipGeo01012021/RipGeo01012021_WGS84.shp'\n",
    "# Read the shape files and create the geodataframes\n",
    "# One for each territorial division (Macroareas, Regions and provinces)\n",
    "Reg_df = gpd.read_file(Reg_Path)\n",
    "Prov_df = gpd.read_file(Prov_Path)\n",
    "Macro_df = gpd.read_file(Macro_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some lists containing the names of problematic territories are created. \n",
    "These lists will be used to create the geodataframes. Indeed, having the right and common nomenclature is needed in order to merge the pandas dataframe with the geopandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalous Regions names\n",
    "Anomalous_Regions = ['Trentino-Alto Adige/Südtirol','Friuli-Venezia Giulia',\"Valle d'Aosta/Vallée d'Aoste\"]\n",
    "#Sud_Sardinia_Agglomeration\n",
    "Sud_Sardinia = ['Ogliastra','Olbia-Tempio','Medio Campidano','Carbonia-Iglesias']\n",
    "# Macro_Areas \n",
    "Macro_Areas = ['Centro', 'Mezzogiorno', 'Nord']\n",
    "# Italy \n",
    "Italy = ['Italia']\n",
    "# Different_Names\n",
    "Different_Names_BES = ['Bolzano/Bozen', 'Forlì-Cesena', 'Massa-Carrara', 'Reggio Calabria']\n",
    "Deffirent_Names_ISTAT = ['Bolzano', \"Forli'-Cesena\", 'Massa Carrara', 'Reggio di Calabria']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following chunk of code we create all the python objects needed to obtain the geodaframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BES_Statistics Dataframe\n",
    "path_ = 'D:/Geospatial_Project/Nuovi_Dati2/'\n",
    "List_Statistics = os.listdir(path = path_)\n",
    "# Read a random file just to compute the provinces afterward\n",
    "df_ = read_dati_bes(path_ + '/' + List_Statistics[0])\n",
    "# Get the Istat regions \n",
    "Regions = Reg_df.DEN_REG.to_list()\n",
    "# Get  BES Regions\n",
    "Bes_Regions = regions_BES(Regions)\n",
    "# Extract the set of all Bes provinces\n",
    "provinces = provinces_BES(df_, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the geodataframe related to provinces. \n",
    "<br>\n",
    "To do so we choose a random datasets from the set of all the dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any datasets relative to the one of the subindicators\n",
    "stat = List_Statistics[0]\n",
    "df = read_dati_bes(path_+ '/' + stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE PROV GEO DF ## \n",
    "Prov_df = clean_prov_geo(Prov_df, provinces)\n",
    "# Create the provinces df\n",
    "df_prov = order_df(df, provinces)\n",
    "# Obtain the full geodataframe of provinces \n",
    "df_prov = from_df_to_gdf(df_prov, Prov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the datasets\n",
    "df_prov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done for provinces we will create our geodataframe relative to the regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk of code does not need to be executed if the one above has been executed\n",
    "\n",
    "# Import any datasets relative to the one of the subindicators\n",
    "stat = List_Statistics[0]\n",
    "df = read_dati_bes(path_+ '/' + stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE REG GEO DF ##\n",
    "Reg_df = mod_col_geo(Reg_df)\n",
    "Reg_df = order_df_regions(Reg_df, Bes_Regions)\n",
    "# Creating Regions DataFrame\n",
    "df_reg = order_df(df, Bes_Regions)\n",
    "# Create the geodaframe used in the representation\n",
    "df_reg = from_df_to_gdf(df_reg, Reg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macroareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will create the macroareas geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE MACRO GEO DF ##\n",
    "Macro_df = aggregate_macros(Macro_df)\n",
    "# 'MACRO-AREA' #\n",
    "# Creating the MacroArea Dataframe\n",
    "df_macro = order_df(df, Macro_Areas)    \n",
    "# Create the geodaframe used in the representation\n",
    "df_macro = from_df_to_gdf(df_macro, Macro_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all the geodataframes, it is possible to create some representation. \n",
    "Two kind of geospatial plots will be created:\n",
    "* A static choropletmap via Matplotlib that will show the data (if specified) grouped in quantiles\n",
    "* An interactive choropletmap created with folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As did before, to avoid long and repetitive chunks of code will be defined some functions at the top that will be then called to create the needed plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_(file_name):\n",
    "    \n",
    "    '''\n",
    "    This function provides the title to plots \n",
    "    '''\n",
    "\n",
    "    title = file_name.strip('.xlsx')\n",
    "    title = title.split('-')\n",
    "    title = title[0].upper() + ' ' + title[1] + ' ' + title[2]\n",
    "\n",
    "    return title \n",
    "\n",
    "def get_labels_(df):\n",
    "    \n",
    "    '''\n",
    "    This functions returns the indicators that will show up in the folium map while using the pointer\n",
    "    over the territories involved in the analysis\n",
    "    '''\n",
    "    \n",
    "    return df.iloc[0]['INDICATORE'] + '\\n' + '(' + df.iloc[0]['UNITA_MISURA'] + ')'\n",
    "\n",
    "def static_choroplet(df, variable, title_, K = 5, Scheme = 'equal_interval'):\n",
    "\n",
    "    '''\n",
    "    With this function is it possible to create a static Choroplet map \n",
    "    Additional feature that can be added are ad-hoc colormaps, and a scheme for the division of the classes.\n",
    "    '''\n",
    "    \n",
    "    miss_df = df[df[variable].isna()]\n",
    "    df.dropna(subset = [variable], inplace = True)\n",
    "    if len(df) > 3:\n",
    "        df = look_for_anomalies2(df, variable)\n",
    "    ax = df.plot(column=variable,\n",
    "                                cmap=\"OrRd\", edgecolor = \"darkgrey\", \n",
    "                                linewidth = 0.9,legend=True,figsize=(14,14),\n",
    "                                scheme=Scheme,\n",
    "                                k=K)\n",
    "    if len(miss_df) > 0 : \n",
    "        miss_df.plot(color = 'gray', ax = ax)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(title_)\n",
    "    #plt.show()\n",
    "    fig = ax.figure\n",
    "    return fig   # modifica 24/04 \n",
    "\n",
    "\n",
    "def dynamic_choroplet(df, title_, measure):\n",
    "    \n",
    "    '''\n",
    "    # NOT USED\n",
    "    With this function it is possible to create an interactive choropletmap\n",
    "    '''\n",
    "\n",
    "    fig = px.choropleth(df,\n",
    "                   geojson=df.geometry,\n",
    "                   locations=df.index, # maybe this should be changed for provinces \n",
    "                   color=measure,\n",
    "                    title = title_)\n",
    "    fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    #fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def folium_interactive_map(df, var, file_name, indicator):\n",
    "    \n",
    "    '''\n",
    "    This function will create an interactive folium choropletmap. Moreover, this maps provides also a feature\n",
    "    that allows to see the specific measure when the reader uses the pointer over the territories involved\n",
    "    in the analysis\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # Create the folium map \n",
    "    m10=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=5)\n",
    "\n",
    "    df = look_for_anomalies2(df, var)\n",
    "    df.to_crs(4326, inplace = True)\n",
    "\n",
    "    \n",
    "    folium.Choropleth(\n",
    "    geo_data = df.to_json(),\n",
    "    data = df,\n",
    "    columns=['TERRITORIO', var],\n",
    "    key_on='feature.properties.TERRITORIO',\n",
    "    #key_on = 'feature.properties.id',\n",
    "    fill_color='Oranges', \n",
    "    fill_opacity=0.6, \n",
    "    line_opacity=1,\n",
    "    nan_fill_color='black',\n",
    "    legend_name= get_title_(file_name),\n",
    "    smooth_factor=0).add_to(m10)\n",
    "\n",
    "\n",
    "    \n",
    "    #add the feature\n",
    "    folium.features.GeoJson(df,\n",
    "                        name='Labels',\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=[var],\n",
    "                                                                aliases = [indicator], #Substitute with the indicator of the df\n",
    "                                                                labels=True,\n",
    "                                                                sticky=False\n",
    "                                                                            )\n",
    "                       ).add_to(m10)\n",
    "    \n",
    "\n",
    "    return m10\n",
    "\n",
    "def look_for_anomalies2(df,var):\n",
    "\n",
    "    '''\n",
    "    This functions is foudnamental to properly create the geospatial repersentations. \n",
    "    Indeed, in the original file dowloaded from the national istitute of statistics, missing values are treated \n",
    "    in different ways: some missing values are reported as a string like that '...', \n",
    "    others are in the format np.nan and lastly some are in the formar math.nan.\n",
    "    This functions simply drops the missing values for the specific year of analysis. \n",
    "    '''\n",
    "\n",
    "    i = 0\n",
    "    in_set = set()\n",
    "\n",
    "    for el in df[var].values:\n",
    "        if type(el) == str or math.isnan(el): \n",
    "            \n",
    "            in_set.add(i)\n",
    "    \n",
    "        i += 1 \n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(list(in_set), inplace = True)   \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not it is possible to create a plot relative to a specific subindicators in a given year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the title\n",
    "titolo = get_title_(stat)\n",
    "# Get_labels \n",
    "labels = get_labels_(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a year in which we are interested to know about\n",
    "y_ = 'V_2016'\n",
    "# Create the static choropletmat for each of the geodaframe\n",
    "matplotlib_fig = static_choroplet(df_macro, variable = y_, title_ = titolo,)\n",
    "matplotlib_fig2 = static_choroplet(df_reg, variable = y_, title_ = titolo)\n",
    "matplotlib_fig3 = static_choroplet(df_prov, variable = y_, title_ = titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folium interactive choroplet maps \n",
    "folium_interactive_map(df_macro, y_, stat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_interactive_map(df_reg, y_, stat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_interactive_map(df_prov, y_, stat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling one of the above functions will display the folium map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above representation show the data for a specific measure in a fixed period of time, it can be useful to create an additional plot, a line chart, that helps to understand the trend of that specific measure. \n",
    "<BR>\n",
    "To do so we will use plotly, a python library that creates interactive plots. \n",
    "Indeed, this kind of chart allows to visualize the data of a specific territory by clicking on the territories in the right slider menù. Depending on the different level of analysis it is possible to decide which observation has to be hided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_chart_plotly(Bes_df, to_hide_1, to_hide_2, to_hide_3, titolo = ''):\n",
    "    \n",
    "    '''\n",
    "    This functions create a dataframe related to a specific measure where each observation refers to a \n",
    "    differnet year. \n",
    "    Then uses this daatframe to create a linechart. \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df_plotly = pd.DataFrame(columns = ['TERRITORIO', 'MISURA', 'ANNO'])\n",
    "    #if len(df_plotly) > 1:\n",
    "    territorio = []\n",
    "    misura = []\n",
    "    _anno_ = []\n",
    "    for i in range(len(Bes_df)):\n",
    "        for j in range(4,20):\n",
    "                \n",
    "            if j < 10:\n",
    "                anno = 'V_200' + str(j)\n",
    "            else:\n",
    "                anno = 'V_20' + str(j)\n",
    "                    \n",
    "            anno_ = int(anno[2:])\n",
    "            territorio.append(Bes_df.iloc[i]['TERRITORIO'])\n",
    "            misura.append(Bes_df.iloc[i][anno])\n",
    "            _anno_.append(anno_)\n",
    "\n",
    "        \n",
    "        \n",
    "    df_plotly['TERRITORIO'] = territorio\n",
    "    df_plotly['ANNO'] = _anno_\n",
    "    df_plotly['MISURA'] = misura\n",
    "\n",
    "\n",
    "    to_hide_1.extend(to_hide_2)\n",
    "    to_hide_1.extend(to_hide_3)\n",
    "    fig = px.line(df_plotly, x = 'ANNO', y = 'MISURA', color = 'TERRITORIO', title = titolo)\n",
    "    fig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n",
    "            if trace.name in to_hide_1 else ())\n",
    "        \n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Macros\n",
    "line_chart_plotly(df, to_hide_1 = provinces, to_hide_2 = Bes_Regions, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regions\n",
    "line_chart_plotly(df, to_hide_1= provinces, to_hide_2 = Macro_Areas, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provinces\n",
    "line_chart_plotly(df, to_hide_1 = Bes_Regions, to_hide_2 = Macro_Areas, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the above choroplets maps, it is possible to create some additional representation that can support the above maps and maybe help to better understand/interpret the data.\n",
    "For this purpose, data from Open street Map will be used to retrieve geodata. For instance, it is possible to collect the data about all the factories in Italy in order to understand if there is some correlation between the presence of many facotries and some indexes related to the enviroment (in the context of pollution) or to the economic wellbeing (many factories are connected to more employment?!). \n",
    "<br>\n",
    "The starting point is dowloading the pbf files of the different regions and or provinces from [Wikimedia Italia](https://osmit-estratti.wmcloud.org/)\n",
    "<br>\n",
    "Secondly there is the data extraction via the osm library called pyrosm. \n",
    "<br>\n",
    "Here it is important to specify two things if someone wants to execut the chunks of code in this seciton of the notebook:\n",
    "* Since some Regions' PBF files were too big, it was infeasible for my local machine to run the code, hence I needed to download the PBFs files of all the provices belonging to these (big) regions. \n",
    "* To run this code it is necessary to have a virtual environment where pyrosm is installed. To do so I created a new envrinoment where I have installed as first package pyrosm. Geopandas was automatcally installed since it is one of the dependencies of pyrosm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style = \"color:red\"> NB: To execute the code in the above cell and the following. Due to problem with conflicting packages I created a venv with geopandas folium etc and onother one where I have just installed pyrosm. By installing pyrosm you also install Geopandas in a compatible version </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#import pyrosm\n",
    "import pandas as pd \n",
    "import  geopandas as gpd\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above chunk of code it is possible to extract points related to many different locations based on a filter that specifies in which kind of places we are interested in and save them into shp or csv files. \n",
    "The tags system is defined by OSM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the one where are saved all the PBFs files dowloaded from Wikimedia Italia\n",
    "os.chdir('D:/Geospatial_Project/Big_Cities')\n",
    "# Create a list with all the pbfs files \n",
    "regions_pbf = os.listdir()\n",
    "custom_filter = {'amenity' : ['srts_centre', 'brothel', 'casino', 'cinema',\n",
    "                            'community_centre', 'conference_centre', 'events_venue', 'fountain',\n",
    "                            'gambling', 'love_hotel', 'nightclub', 'planetarium', 'public_bookcase',\n",
    "                            'social_centre', 'stripclub', 'studio', 'swingerclub', 'theatre']}\n",
    "# Define the filter\n",
    "\n",
    "\n",
    "# Iterate along all the regions (or provinces)\n",
    "for region in regions_pbf:\n",
    "    \n",
    "    name = region.split('_')\n",
    "    name = name[1]\n",
    "    \n",
    "    osm = pyrosm.OSM(region)\n",
    "    data = osm.get_pois(custom_filter = custom_filter)\n",
    "    \n",
    "    try:\n",
    "        data.to_file('D:/Geospatial_Project/Freetime/' + name + '/' + name + '.shp')\n",
    "    except:\n",
    "        data = pd.DataFrame(data)\n",
    "        data.to_csv('D:/Geospatial_Project/Freetime/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_shp(path):\n",
    "\n",
    "    '''\n",
    "    This function read the csv and shp files that contain the geodata retrived from osm.\n",
    "    Then it merges all this files into a big datasets where are stored the latitude, longitude \n",
    "    and the name of the point of interest.    \n",
    "    '''\n",
    "    \n",
    "    _full_df_ = gpd.GeoDataFrame(crs = 4326, columns = ['name', 'lat', 'lon'])\n",
    "    lats = []\n",
    "    lons = []\n",
    "    names = []\n",
    "    list_files = os.listdir(path)\n",
    "    list_files_ = []\n",
    "\n",
    "\n",
    "    for file_ in list_files:\n",
    "        if file_.endswith('.csv'):\n",
    "            list_files_.append(file_)\n",
    "        elif file_.endswith('.shp'):\n",
    "            list_files_.append(file_)\n",
    "\n",
    "\n",
    "    for _file_ in list_files_:\n",
    "        \n",
    "        if _file_.endswith('.csv'):\n",
    "            _df_ = pd.read_csv(path + '/' + _file_)\n",
    "            if len(_df_) > 0:\n",
    "                _df_['geometry'] = gpd.GeoSeries.from_wkt(_df_['geometry'])\n",
    "                _geodf_ = gpd.GeoDataFrame(_df_, crs = 4326)\n",
    "        \n",
    "        elif _file_.endswith('.shp'):\n",
    "            _geodf_ = gpd.read_file(path + '/' + _file_)\n",
    "        lats.extend(_geodf_.geometry.centroid.to_crs(4326).y)\n",
    "        lons.extend(_geodf_.geometry.centroid.to_crs(4326).x)\n",
    "\n",
    "        if 'name' in _geodf_.columns:\n",
    "            names.extend(_geodf_.name)\n",
    "        else: \n",
    "            names.extend(['Missing'] * len(_geodf_.geometry.centroid.to_crs(4326).x))\n",
    "    \n",
    "\n",
    "    new_names = []\n",
    "    for n in names: \n",
    "        if type(n) != str:\n",
    "            new_names.append('Missing')\n",
    "        else:\n",
    "            new_names.append(n)\n",
    "    \n",
    "\n",
    "    _full_df_['lat'] = lats\n",
    "    _full_df_['lon']= lons\n",
    "    _full_df_['name'] = new_names\n",
    "\n",
    "    return _full_df_\n",
    "\n",
    "def getMarker(lat,lon, message,inconstyle):\n",
    "\n",
    "    '''\n",
    "    This function simply add a marker on a folium map based on the point latitude and logitude. \n",
    "    In addition, there is also a pop up message that shows up when the user interact with the map. \n",
    "    Lastly, it is even possible to specify an icon that defines the point on a map. \n",
    "    '''\n",
    "\n",
    "    marker = folium.Marker(location=[lat,lon],\n",
    "                         popup=message,\n",
    "                         icon=inconstyle)\n",
    "    return marker\n",
    "\n",
    "\n",
    "def marker_plot(df, logo : str):\n",
    "    '''\n",
    "    This functions create a map. \n",
    "    Then it adds to this maps all the points present in the given datasets. \n",
    "    By iterating over the rwos of the provided dataset applies the function defined above \n",
    "    (getMarker) to add the marker. \n",
    "    '''\n",
    "\n",
    "    m5=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    for index, row in df.iterrows():\n",
    "        #icon=folium.Icon(color='purple',prefix='fa',icon='arrow-circle-down')\n",
    "        icon=folium.features.CustomIcon(logo, icon_size=(34,34))\n",
    "        marker = getMarker(row['lat'],row['lon'],row['name'], icon)\n",
    "\n",
    "        marker.add_to(m5)\n",
    "    \n",
    "    return  m5\n",
    "\n",
    "def marker_cluster(df, logo:str):\n",
    "\n",
    "    '''\n",
    "    This function creates a map where all the points are added, in the same way of the function marker plot,\n",
    "    but it also aggregate into cluster closer points. This makes the map more good looking. \n",
    "    Moreover, by zooming in into the map the cluser become smaller and smaller until it the cluster is made by\n",
    "    just one point.  \n",
    "    '''\n",
    "\n",
    "    m6=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    marker_cluster = MarkerCluster().add_to(m6)\n",
    "    for index, row in df.iterrows():\n",
    "        #icon=folium.Icon(color='purple',prefix='fa',icon='arrow-circle-down')\n",
    "        icon=folium.features.CustomIcon(logo, icon_size=(34,34))\n",
    "        message = '<strong>sezionD:'+ str(row['name'])\n",
    "        #tip = message + '<br/>' + row['via']\n",
    "        marker = getMarker(row['lat'],row['lon'],message, icon)\n",
    "        #add to marker cluster \n",
    "        marker.add_to(marker_cluster)\n",
    "    \n",
    "    \n",
    "    return m6\n",
    "\n",
    "def heatmap_plot(df):\n",
    "\n",
    "    '''\n",
    "    This function creates a map where we can visualize the distribution of points via a \n",
    "    heatmap. Indeed the more the color is closer to red the higher is the concetration of points\n",
    "    in that area.  \n",
    "    '''\n",
    "\n",
    "    m7 = folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    data = df[['lat','lon']]\n",
    "    HeatMap(data.values).add_to(m7)\n",
    "\n",
    "\n",
    "    return m7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in the following chunk of code two maps concerning the italian factories in the italian territory are created: \n",
    "* the first is a map where factories are aggregated into clusters,\n",
    "* the second is an heatmap showing the distribution of factories along the italian territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = read_csv_shp('D:\\Geospatial_Project\\Factories')\n",
    "marker_cluster(_df_, logo = 'D:/Geospatial_Project/logo2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_plot(_df_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same as before but for No-profit organizations in Italy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = read_csv_shp('D:/Geospatial_Project/No_Profit')\n",
    "marker_cluster(_df_, logo = 'D:/Geospatial_Project/np2.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_plot(_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  __Tobler's first law of geography:__  _Everything is related to everything else, but near things are more related than distant things (Tobler, 1970)_ \n",
    "\n",
    "\n",
    "In this section of the notebook we introduce methods of exploratory spatial data analysis that are intended to complement geovisualization through formal univariate and multivariate statistical tests for spatial autocorrelation.\n",
    "The term _spatial auto-correlation_ refers to the presence of systematic spatial variation in a mapped variable. Where adjacent observations have similar data values, the map will show positive spatial autocorrelation. Where the adjacent observations have very contrasting values then the map will show negative spatial auto-correlation.\n",
    "<br>\n",
    "In this example, there will be shown the results relatively to the networth pro capite. \n",
    "<br>\n",
    "NB: This measure has been chosen because ther is a high correaltion based on the loction of the territory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define both the geodataframe at regional and provincial level. \n",
    "Then we will merge these two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_Path = 'D:/Geospatial_Project/Dati/Limiti/Lim/Reg01012021/Reg01012021_WGS84.shp'\n",
    "Prov_Path = 'D:/Geospatial_Project/Dati/Limiti/Lim/ProvCM01012021/ProvCM01012021_WGS84.shp'\n",
    "# Reading the geodaframe\n",
    "\n",
    "Reg_df = gpd.read_file(Reg_Path)\n",
    "Prov_df = gpd.read_file(Prov_Path)\n",
    "\n",
    "\n",
    "# Anomalous Regions names\n",
    "Anomalous_Regions = ['Trentino-Alto Adige/Südtirol','Friuli-Venezia Giulia',\"Valle d'Aosta/Vallée d'Aoste\"]\n",
    "#Sud_Sardinia_Agglomeration\n",
    "Sud_Sardinia = ['Ogliastra','Olbia-Tempio','Medio Campidano','Carbonia-Iglesias']\n",
    "# Macro_Areas \n",
    "Macro_Areas = ['Centro', 'Mezzogiorno', 'Nord']\n",
    "# Italy \n",
    "Italy = ['Italia']\n",
    "# Different_Names\n",
    "Different_Names_BES = ['Bolzano/Bozen', 'Forlì-Cesena', 'Massa-Carrara', 'Reggio Calabria']\n",
    "Deffirent_Names_ISTAT = ['Bolzano', \"Forli'-Cesena\", 'Massa Carrara', 'Reggio di Calabria']\n",
    "\n",
    "# Read the BES_Statistics Dataframe\n",
    "path_ = 'D:/Geospatial_Project/Nuovi_Dati2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Istat regions \n",
    "Regions = Reg_df.DEN_REG.to_list()\n",
    "# Get  BES Regions\n",
    "Bes_Regions = regions_BES(Regions)\n",
    "# Extract the set of all Bes provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE REG GEO DF ##\n",
    "Reg_df = mod_col_geo(Reg_df)\n",
    "Reg_df = order_df_regions(Reg_df, Bes_Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = read_dati_bes('Ambiente-Disponibilità di verde urbano-Totale-m2 per abitante.xlsx')\n",
    "df = read_dati_bes(path_ + 'Benessere economico-Patrimonio pro capite-Totale-euro.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = provinces_BES(df, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE PROV GEO DF ## \n",
    "Prov_df = clean_prov_geo(Prov_df, provinces)\n",
    "# 'PROVINCES' #\n",
    "# Create the provinces df\n",
    "df_prov = order_df(df, provinces)\n",
    "# Obtain the full geodataframe of provinces \n",
    "df_prov = from_df_to_gdf(df_prov, Prov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the crs in order to properly plot the geometries. \n",
    "df_prov.to_crs(4326, inplace = True)\n",
    "Reg_df.to_crs(4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prov2 = look_for_anomalies2(df_prov, 'V_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prov2 = df_prov2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the two dataframes.\n",
    "prova_df = gpd.sjoin(Reg_df, \n",
    "                          df_prov2, how='inner', predicate='contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova_df2 = prova_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average value by grouping provinces by their region (in 2017)\n",
    "median_val = prova_df2['V_2017'].groupby([prova_df2['Reg']]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the mean value of the networth pro-capite in the different regions. \n",
    "median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new aggregated dataframe\n",
    "prova_df2 = prova_df.merge(median_val, on = 'Reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a simple choroplet map in order to have a first visualization of the data.\n",
    "prova_df2.plot(column = 'V_2017_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reate a more sophisticated choroplet maps \n",
    "#that divides the region (based on the networth pro-capite in 2017) in 5 groups.\n",
    "fig, ax = plt.subplots(figsize=(12,10), subplot_kw={'aspect':'equal'})\n",
    "prova_df2.plot(column='V_2017_y', scheme='Quantiles', k=5, cmap='GnBu', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spatial Autocorrelation:__\n",
    "\n",
    "Visual inspection of the map pattern for the pro-capite networth allows us to search for spatial structure. If the spatial distribution of the networth was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the lighter clusters in the south as well as the center, and a concentration of the darker hues (higher networth pro capite) in the north. \n",
    "\n",
    "Our brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes ([Rey,2022](https://sergerey.org/)) ([Wolf, 2022](https://www.ljwolf.org/posts/2020-sm2-lectures/)).\n",
    "\n",
    "The concept of _spatial autocorrelation_ relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\n",
    "\n",
    "Let's use [PySAL](https://pysal.org/) to generate these two types of similarity measures.\n",
    "\n",
    "__Spatial Similarity__\n",
    "In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prova_df2\n",
    "wq =  lps.weights.Queen.from_dataframe(df)\n",
    "wq.transform = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Similarity\n",
    "So the spatial weight between neighborhoods i and j indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood i the spatial lag is defined as:\n",
    "<br>\n",
    "                                   <h3><center>$ylag_{i}=\\sum_j{w_{i,j}y_{j}}$</center></h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['V_2017_y']\n",
    "ylag = lps.weights.lag_spatial(wq, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapclassify as mc\n",
    "ylagq5 = mc.Quantiles(ylag, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=ylagq5.yb).plot(column='cl', categorical=True, \\\n",
    "        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.title(\"Spatial Lag Median Networth pro-capite (Quintiles)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lag_median_val'] = ylag\n",
    "f,ax = plt.subplots(1,2,figsize=(2.16*4,4))\n",
    "df.plot(column='V_2017_y', ax=ax[0], edgecolor='k',\n",
    "        scheme=\"quantiles\",  k=5, cmap='GnBu')\n",
    "ax[0].axis(df.total_bounds[np.asarray([0,2,1,3])])\n",
    "ax[0].set_title(\"Networth pro-capite\")\n",
    "df.plot(column='lag_median_val', ax=ax[1], edgecolor='k',\n",
    "        scheme='quantiles', cmap='GnBu', k=5)\n",
    "ax[1].axis(df.total_bounds[np.asarray([0,2,1,3])])\n",
    "ax[1].set_title(\"Spatial Lag Neworth pro-capite\")\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still have the challenge of visually associating the value of the networth in a region with the value of the spatial lag of values for the focal unit. The latter is a weighted average of list networths in the focal region.\n",
    "\n",
    "To complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Global Spatial Autocorrelation__\n",
    "We begin with a simple case where the variable under consideration is binary. This is useful to unpack the logic of spatial autocorrelation tests. So even though our attribute is a continuously valued one, we will convert it to a binary case to illustrate the key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y > y.median()\n",
    "sum(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y > y.median()\n",
    "labels = [\"0 Low\", \"1 High\"]\n",
    "yb = [labels[i] for i in 1*yb] \n",
    "df['yb'] = yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10), subplot_kw={'aspect':'equal'})\n",
    "df.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Join counts__\n",
    "One way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called _joins_. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\n",
    "\n",
    "Each unit can take on one of two values \"Black\" or \"White\", and so for a given pair of neighboring locations there are three different types of joins that can arisD:\n",
    "\n",
    "Black Black (BB)\n",
    "White White (WW)\n",
    "Black White (or White Black) (BW)\n",
    "Given that we have 47 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map? This is the logic of join count statistics.\n",
    "\n",
    "We can use the ´esda´ package from ´PySAL´ to carry out join count analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esda \n",
    "yb = 1 * (y > y.median()) # convert back to binary\n",
    "wq =  lps.weights.Queen.from_dataframe(df)\n",
    "wq.transform = 'b'\n",
    "np.random.seed(12345)\n",
    "jc = esda.join_counts.Join_Counts(yb, wq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results of the join analysis\n",
    "import seaborn as sbn\n",
    "sbn.kdeplot(jc.sim_bb, shade=True)\n",
    "plt.vlines(jc.bb, 0, 0.075, color='r')\n",
    "plt.vlines(jc.mean_bb, 0,0.075)\n",
    "plt.xlabel('BB Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for the networth procapite. Clearly our observed value is extremely high. A pseudo p-value summarizes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the pseudo p-value\n",
    "jc.p_sim_bb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in networth pro-capite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Continuous Case__\n",
    "The join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\n",
    "\n",
    "First, we transform our weights to be row-standardized, from the current binary state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.transform = 'r'\n",
    "y = df['V_2017_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess whether spatial auto-correlation is statistically significant, it is necessary to perfrom the Moran I statistic. In essence, it is a cross-product statistic between a variable and its spatial lag, with the variable expressed in deviations from its mean. For an observation, at location i, this is expressed as\n",
    "\n",
    "$z_i=x_i − \\bar{x}$, where $\\bar{x}$ is the mean of variable $x$.\n",
    "\n",
    "Moran’s I statistic is then:\n",
    "$ I = \\frac{{}\\sum_{i} \\sum_{j} w_{ij} * z_i * z_j /{S_0}}{\\sum_{i} z_{i}^2 /{n}}$\n",
    "\n",
    "\n",
    "with $w_{ij}$ as the elements of the spatial weights matrix, $S_0 = \\sum_i \\sum_j w_{ij}$ as the sum of all the weights, and $n$ as the number of observations. \n",
    "\n",
    "Inference for Moran’s I is based on a null hypothesis of spatial randomness. The distribution of the statistic under the null can be derived using either an assumption of normality (independent normal random variates), or so-called randomization (i.e., each value is equally likely to occur at any location). While the analytical derivations provide easy to interpret expressions for the mean and the variance of the statistic under the null hypothesis, inference based on them employs an approximation to a standard normal distribution, which may be inappropriate when the underlying assumptions are not satisfied\n",
    "\n",
    "Moran's I is a test for global autocorrelation for a continuous attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "mi = esda.moran.Moran(y, wq)\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.kdeplot(mi.sim, shade=True)\n",
    "plt.vlines(mi.I, 0, 1, color='r')\n",
    "plt.vlines(mi.EI, 0,1)\n",
    "plt.xlabel(\"Moran's I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Local Autocorrelation: Hot Spots, Cold Spots, and Spatial Outliers__\n",
    "In addition to the Global autocorrelation statistics, PySAL has many local autocorrelation statistics. Let's compute a local Moran statistic for the same d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.transform = 'r'\n",
    "lag_price = lps.weights.lag_spatial(wq, df['V_2017_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moran scatter plot consists of a plot with the spatially lagged variable on the y-axis and the original variable on the x-axis. The slope of the linear fit to the scatter plot equal Moran's I. A variable $z$ is considered, given in deviations from the mean. With row-standardized weights, the sum of all the weights ($S_0$) equals the number of observations ($n$). As a result, the expression for Moran's I simplifies to:\n",
    "$I =  \\frac{{}\\sum_{i} \\sum_{j} w_{ij} * z_i * z_j /{S_0}}{\\sum_{i} z_{i}^2 /{n}} =  \\frac{\\sum_{i} (z_i X \\sum_j w_{ij}z_j)} {\\sum_i z_i^2}$\n",
    "This turns out to be the slope of a regression of $\\sum_j w_{ij}z_j$ on $z_i$. This is the principle underlying the Moran scatter plot.\n",
    "\n",
    "An important aspect of the visualization in the Moran scatter plot is the classification of the nature of spatial auto-correlation into four categories. Since the plot is centered on the mean (of zero), all points to the right of the mean have $z_i>0$ and all points to the left have $z_i<0$. We refer to these values respectively as high and low, in the limited sense of higher or lower than average. Similarly, we can classify the values for the spatial lag above and below the mean as high and low.\n",
    "\n",
    "The scatter plot is then easily decomposed into four quadrants. The upper-right quadrant and the lower-left quadrant correspond with positive spatial auto-correlation (similar values at neighboring locations). We refer to them as respectively high-high and low-low spatial auto-correlation. In contrast, the lower-right and upper-left quadrant correspond to negative spatial auto-correlation (dissimilar values at neighboring locations). We refer to them as respectively high-low and low-high spatial auto-correlation.\n",
    "\n",
    "The classification of the spatial auto-correlation into four types begins to make the connection between global and local spatial auto-correlation. However, it is important to keep in mind that the classification as such does not imply significance. This is further explored in our discussion of local indicators of spatial association (LISA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df['V_2017_y']\n",
    "b, a = np.polyfit(price, lag_price, 1)\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "\n",
    "plt.plot(price, lag_price, '.', color='firebrick')\n",
    "\n",
    " # dashed vert at mean of the price\n",
    "plt.vlines(price.mean(), lag_price.min(), lag_price.max(), linestyle='--')\n",
    " # dashed horizontal at mean of lagged price \n",
    "plt.hlines(lag_price.mean(), price.min(), price.max(), linestyle='--')\n",
    "\n",
    "# red line of best fit using global I as slope\n",
    "plt.plot(price, a + b*price, 'r')\n",
    "plt.title('Moran Scatterplot')\n",
    "plt.ylabel('Spatial Lag of networth pro-capite')\n",
    "plt.xlabel('Networth pro-capite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of a single I statistic, we have an array of local Ii statistics, stored in the ´.Is´ attribute, and p-values from the simulation are ´in p_sim´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = esda.moran.Moran_Local(y, wq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can distinguish the specific type of local spatial association reflected in the four quadrants of the Moran Scatterplot abovD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = li.p_sim < 0.05\n",
    "hotspot = sig * li.q==1\n",
    "coldspot = sig * li.q==3\n",
    "doughnut = sig * li.q==2\n",
    "diamond = sig * li.q==4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'hot spot']\n",
    "labels = [spots[i] for i in hotspot*1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['red', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'cold spot']\n",
    "labels = [spots[i] for i in coldspot*1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['blue', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 1 * (li.p_sim < 0.05)\n",
    "hotspot = 1 * (sig * li.q==1)\n",
    "coldspot = 3 * (sig * li.q==3)\n",
    "doughnut = 2 * (sig * li.q==2)\n",
    "diamond = 4 * (sig * li.q==4)\n",
    "spots = hotspot + coldspot + doughnut + diamond\n",
    "spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_labels = [ '0 ns', '1 hot spot', '2 doughnut', '3 cold spot', '4 diamond']\n",
    "labels = [spot_labels[i] for i in spots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap([ 'lightgrey', 'red', 'lightblue', 'blue', 'pink'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this last plot, there is a clear division among the south and the north of Italy for what concerns the net woth pro-capita: Northern regions habitants have an higher net worth pro-capita than the ones of southern regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isochrone map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have many data for each indicator, the data related to the _Subjective wellbeing_ are totally missing. \n",
    "<br>\n",
    "According to the [BES](https://www.istat.it/it/files/2018/04/12-domains-scientific-commission.pdf) this indicator relies on four sub-indicators:\n",
    "1) Life satisfaction\n",
    "2) Leaisure time satisfaction \n",
    "3) Positive judgments for future perspective\n",
    "4) Negative judgments for future perspecitve\n",
    "<br>\n",
    "\n",
    "\n",
    "It is quite difficult to find this kind of data without having the results of ad-hoc surveys constructed for this specific aim. Moreover, it is also difficult to find some other index/indicators that can approximate them especially if we work with geodata. \n",
    "<br>\n",
    "\n",
    "However, could be interesting to see how many amenties can be reached in 10-20minutes (isochrone maps) from the city center. Indeed, having this kind of representation could help us to understand something about the second sub-indicator: Leisure time satisfaction. As a matter of fact, having many pubs, cinema and other places where it is possible to spend your free time can be positively correlated to have a high leisure-time satisfaction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openrouteservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "import openrouteservice as ors\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk of code we take advantage of the geocoder to extract the representative points of the 10 biggest italian cities.\n",
    "<br>\n",
    "NB: we consider just the first 10 cities to obtain a clearer map. For instance, if we decided to collect the amenities for all the provinces of italy, then the map would have been too messy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city']\n",
    "names = [('Roma'),('Torino'),('Genova'),('Bari'), ('Milano'), ('Genova'), ('Palermo'), ('Napoli'), ('Bologna'), ('Verona')]\n",
    "cities = gpd.GeoDataFrame(names,columns=cols)\n",
    "geo_cities = gpd.tools.geocode(cities.city, provider=\"arcgis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As did before, we use osm to extract the location of all the amanities related to freetime activities and entratainment in the required places. \n",
    "Then through the already mentioned function ´read_csv_shp´ we create a dataframe with the coordinates and names of these facilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = read_csv_shp('Freetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we firstly create a folium marker cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = marker_cluster(complete_df, 'D:\\Geospatial_Project\\pointer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk of code a customized pointer with the logo of each city is added to the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in geo_cities.iterrows():\n",
    "      icon=folium.features.CustomIcon(os.getcwd() + '/Icons/' + row['address']+ '.png',icon_size=(40,40))\n",
    "marker = getMarker(row['geometry'].centroid.y,row['geometry'].centroid.x, row['address'],row['address'], icon)\n",
    "marker.add_to(m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we had the isochrone maps that highlight the facilities that can be reached in 10-20minutes of walk having as starting point the representative point of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ors_key = '5b3ce3597851110001cf6248d3c5776fac4c4918b5258bb3659ddec6'\n",
    "client = ors.Client(key=ors_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(geo_cities)):\n",
    "  city = geo_cities.iloc[i]\n",
    "  coordinate = [[city.geometry.centroid.x, city.geometry.centroid.y]]\n",
    "  iscorhrone = client.isochrones( locations=coordinate,\n",
    "    profile='foot-walking',\n",
    "    range=[600, 1200],\n",
    "    validate=False,)\n",
    "  popup_message = f'outline shows areas reachable within 10-20 minutes'\n",
    "  folium.GeoJson(iscorhrone, name='isochrone', tooltip= popup_message).add_to(m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the map\n",
    "m5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Territories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last stage of the analysis is trying to understand which are the best areas for each of the 12 subindicators and also the best territory over all. \n",
    "<br>\n",
    "To do a simple model has been adopted: \n",
    "1) Firstly, has been created a dictionary that has as keys all the subindicators and as value  it has ´True´ when the higher the value of the subindicator the better it is (e.g. amount of energy coming from renewable sources) and ´False´ when the lower the value the better it is. \n",
    "2) Secondly, for each level of analysis (Provinces, Regions, and Macroareas), a dictionary that will store the results has been createed. These dictionaries have as key the territories' name and as value another dictionary. This second dictionary has as keys the twelve subindicators and as values an empty list.\n",
    "3) For each level of analysis and for all indicator, the territories are sorted according to the measure (in asceding order if the subindicator is positive and on contrary in descending order). \n",
    "4) A rank is assigned to each territory based on the order. \n",
    "5) All these rankings are stored in the subdictionary\n",
    "6) For each indicator, the best territories are the ones whose sum of the subindicators' raking is the lower\n",
    "7) The overall best territories are the ones which have the lowest sum of rakings among all subindicators. \n",
    "\n",
    "NB: The rakings are based on the most recent data available. \n",
    "\n",
    "NB: This way of assessing the best territories is obviously an extreme approximation of the reality since we just consider the subindicators and we consider them in an additive way, without weighting the importance of them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary tells us whether it is better to have an higher value for the indicator or not.\n",
    "\n",
    "diz_sub = {'Benessere economico-Importo medio annuo pro_capite dei redditi pensionistici-Femmine-euro.xlsx': True,\n",
    " 'Benessere economico-Importo medio annuo pro_capite dei redditi pensionistici-Maschi-euro.xlsx': True,\n",
    " 'Benessere economico-Importo medio annuo pro_capite dei redditi pensionistici-Totale-euro.xlsx': True,\n",
    " 'Benessere economico-Patrimonio pro capite-Totale-euro.xlsx': True,\n",
    " 'Benessere economico-Pensionati con pensione di basso importo-Femmine-valori percentuali.xlsx': False,\n",
    " 'Benessere economico-Pensionati con pensione di basso importo-Maschi-valori percentuali.xlsx': False,\n",
    " 'Benessere economico-Pensionati con pensione di basso importo-Totale-valori percentuali.xlsx': False,\n",
    " 'Benessere economico-Reddito medio disponibile pro capite-Totale-euro.xlsx': True,\n",
    " 'Benessere economico-Retribuzione media annua dei lavoratori dipendenti-Femmine-euro.xlsx': True,\n",
    " 'Benessere economico-Retribuzione media annua dei lavoratori dipendenti-Maschi-euro.xlsx': True,\n",
    " 'Benessere economico-Retribuzione media annua dei lavoratori dipendenti-Totale-euro.xlsx': True,\n",
    " 'Benessere economico-Tasso di ingresso in sofferenza dei prestiti bancari alle famiglie-Totale-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza alfabetica non adeguata-Femmine-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza alfabetica non adeguata-Maschi-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza alfabetica non adeguata-Totale-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza numerica non adeguata-Femmine-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza numerica non adeguata-Maschi-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Competenza numerica non adeguata-Totale-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Giovani che non lavorano e non studiano (Neet)-Totale-valori percentuali.xlsx': False,\n",
    " 'Istruzione e formazione-Laureati e altri titoli terziari (25-39 anni)-Totale-valori percentuali.xlsx': True,\n",
    " 'Istruzione e formazione-Partecipazione alla formazione continua-Totale-valori percentuali.xlsx': True,\n",
    " \"Istruzione e formazione-Passaggio all'università-Totale-valori percentuali (tasso specifico di coorte).xlsx\": True,\n",
    " 'Istruzione e formazione-Persone con almeno il diploma (25-64 anni)-Totale-valori percentuali.xlsx': True,\n",
    " 'Ambiente-Conferimento dei rifiuti urbani in discarica-Totale-valori percentuali.xlsx': False,\n",
    " 'Ambiente-Dispersione da rete idrica comunale-Totale-valori percentuali.xlsx': False,\n",
    " 'Ambiente-Disponibilità di verde urbano-Totale-m2 per abitante.xlsx': True,\n",
    " 'Ambiente-Energia elettrica da fonti rinnovabili-Totale-valori percentuali.xlsx': True,\n",
    " 'Ambiente-Impermeabilizzazione del suolo da copertura artificiale-Totale-valori percentuali.xlsx': True,\n",
    " \"Ambiente-Qualità dell'aria urbana - Biossido di azoto-Totale-valori percentuali.xlsx\": False,\n",
    " \"Ambiente-Qualità dell'aria urbana - PM10-Totale-valori percentuali.xlsx\": False,\n",
    " 'Ambiente-Raccolta differenziata dei rifiuti urbani-Totale-valori percentuali.xlsx': True,\n",
    " 'Salute-Mortalità infantile-Totale-per 1.000 nati vivi.xlsx': False,\n",
    " 'Salute-Mortalità per demenze e malattie del sistema nervoso (65 anni e più)-Femmine-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per demenze e malattie del sistema nervoso (65 anni e più)-Maschi-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per demenze e malattie del sistema nervoso (65 anni e più)-Totale-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per incidenti stradali (15-34 anni)-Femmine-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per incidenti stradali (15-34 anni)-Maschi-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per incidenti stradali (15-34 anni)-Totale-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per tumore (20-64 anni)-Femmine-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per tumore (20-64 anni)-Maschi-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Mortalità per tumore (20-64 anni)-Totale-tassi standardizzati per 10.000 residenti.xlsx': False,\n",
    " 'Salute-Speranza di vita alla nascita-Femmine-numero medio di anni.xlsx': True,\n",
    " 'Salute-Speranza di vita alla nascita-Maschi-numero medio di anni.xlsx': True,\n",
    " 'Salute-Speranza di vita alla nascita-Totale-numero medio di anni.xlsx': True,\n",
    " 'Innovazione, ricerca e creatività-Addetti nelle imprese culturali-Totale-valori percentuali.xlsx': True,\n",
    " 'Innovazione, ricerca e creatività-Mobilità dei laureati italiani (25-39 anni)-Totale-per 1.000 laureati residenti.xlsx': True,\n",
    " 'Innovazione, ricerca e creatività-Propensione alla brevettazione-Totale-per milioni di abitanti.xlsx': True,\n",
    " 'Paesaggio e patrimonio culturale-Densità di verde storico-Totale-per 100 m2.xlsx': True,\n",
    " 'Paesaggio e patrimonio culturale-Densità e rilevanza del patrimonio museale-Totale-per 100 km2.xlsx': True,\n",
    " 'Paesaggio e patrimonio culturale-Diffusione delle aziende agrituristiche-Totale-per 100 km2.xlsx': True,\n",
    " 'Politica e istituzioni-Affollamento degli istituti di pena-Totale-valori percentuali.xlsx': False,\n",
    " 'Politica e istituzioni-Amministratori comunali con meno di 40 anni-Totale-valori percentuali.xlsx': True,\n",
    " 'Politica e istituzioni-Amministratori comunali donne-Totale-valori percentuali.xlsx': True,\n",
    " 'Politica e istituzioni-Partecipazione elettorale (elezioni regionali)-Totale-valori percentuali.xlsx': True,\n",
    " 'Politica e istituzioni-Partecipazione elettorale-Totale-valori percentuali.xlsx': True,\n",
    " \"Qualità dei servizi-Bambini che hanno usufruito dei servizi comunali per l'infanzia-Totale-valori percentuali.xlsx\": True,\n",
    " 'Qualità dei servizi-Emigrazione ospedaliera in altra regione-Totale-valori percentuali.xlsx': False,\n",
    " 'Qualità dei servizi-Irregolarità del servizio elettrico-Totale-numero medio per utente.xlsx': False,\n",
    " 'Qualità dei servizi-Posti km offerti dal Tpl-Totale-valori per abitante.xlsx': True,\n",
    " 'Sicurezza-Altri delitti violenti denunciati-Totale-per 10.000 abitanti.xlsx': False,\n",
    " 'Sicurezza-Delitti diffusi denunciati-Totale-per 10.000 abitanti.xlsx': False,\n",
    " 'Sicurezza-Mortalità stradale in ambito extraurbano-Totale-valori percentuali.xlsx': False,\n",
    " 'Sicurezza-Omicidi-Totale-per 100.000 abitanti.xlsx': False,\n",
    " 'Relazioni sociali-Organizzazioni non profit-Totale-per 10.000 abitanti.xlsx': True,\n",
    " 'Relazioni sociali-Scuole accessibili-Totale-valori percentuali.xlsx': True,\n",
    " \"Lavoro e conciliazione dei tempi di vita-Giornate retribuite nell'anno (lavoratori dipendenti)-Totale-valori percentuali.xlsx\": True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di infortuni mortali e inabilità permanente-Femmine-per 10.000 occupati.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di infortuni mortali e inabilità permanente-Maschi-per 10.000 occupati.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di infortuni mortali e inabilità permanente-Totale-per 10.000 occupati.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro giovanile (15-29 anni)-Femmine-valori percentuali.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro giovanile (15-29 anni)-Maschi-valori percentuali.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro giovanile (15-29 anni)-Totale-valori percentuali.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro-Femmine-valori percentuali.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro-Maschi-valori percentuali.xlsx': False,\n",
    " \"Lavoro e conciliazione dei tempi di vita-Giornate retribuite nell'anno (lavoratori dipendenti)-Femmine-valori percentuali.xlsx\": True,\n",
    " \"Lavoro e conciliazione dei tempi di vita-Giornate retribuite nell'anno (lavoratori dipendenti)-Maschi-valori percentuali.xlsx\": True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di mancata partecipazione al lavoro-Totale-valori percentuali.xlsx': False,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione (20-64 anni)-Femmine-valori percentuali.xlsx': True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione (20-64 anni)-Maschi-valori percentuali.xlsx': True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione (20-64 anni)-Totale-valori percentuali.xlsx': True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione giovanile (15-29 anni)-Femmine-valori percentuali.xlsx': True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione giovanile (15-29 anni)-Maschi-valori percentuali.xlsx': True,\n",
    " 'Lavoro e conciliazione dei tempi di vita-Tasso di occupazione giovanile (15-29 anni)-Totale-valori percentuali.xlsx': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary where all the rakings will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_prov = dict()\n",
    "for prov in provinces:\n",
    "    classes = os.listdir('D:\\Geospatial_Project\\Dati_Streamlite')\n",
    "    classes.remove('Wk_2')\n",
    "    classes.remove('Subjective well-being')\n",
    "    diz_ind = dict()\n",
    "    for folder in classes:\n",
    "        diz_ind[folder] = []\n",
    "                \n",
    "    diz_prov[prov] = diz_ind "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the rankings.\n",
    "\n",
    "NB: This operation needs to be repeateed for each indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "\n",
    "for i in range(4,20):\n",
    "    if len(str(i)) == 1: \n",
    "        data = 'V_200' + str(i)\n",
    "    else:\n",
    "        data = 'V_20' + str(i) \n",
    "\n",
    "    years.append(data)\n",
    "\n",
    "years.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'D:\\Geospatial_Project\\Dati_Streamlite\\\\'\n",
    "ind = 'Work & Life Balance'\n",
    "for sub_ind in os.listdir(dir+ind):\n",
    "    # Read the geodataframe\n",
    "    Prov_df = gpd.read_file(Prov_Path)\n",
    "    df = read_dati_bes(dir+ind+ '\\\\' + sub_ind)\n",
    "    ## PREPARE PROV GEO DF ## \n",
    "    Prov_df = clean_prov_geo(Prov_df, provinces)\n",
    "    # Create the provinces df\n",
    "    df_prov = order_df(df, provinces)\n",
    "    # Obtain the full geodataframe of provinces \n",
    "    df_prov = from_df_to_gdf(df_prov, Prov_df)\n",
    "    # reset the index\n",
    "    df_prov.reset_index(inplace = True) \n",
    "    # check the data for the most recent year\n",
    "    for y_ in years: \n",
    "        if not df_prov[y_].isna().all():\n",
    "            break\n",
    "    \n",
    "    \n",
    "    df_prov = look_for_anomalies2(df_prov, y_)\n",
    "    if diz_sub[sub_ind]: # check whether having an high value is good or not \n",
    "        df_prov.sort_values(by = y_, inplace= True, ascending= False)\n",
    "    else:\n",
    "        df_prov.sort_values(by = y_, inplace = True)\n",
    "    \n",
    "    df_prov['Rank'] = list(range(1, len(df_prov)+1))\n",
    "\n",
    "    for i in range(len(df_prov)):\n",
    "        territorio = df_prov.iloc[i]['TERRITORIO']\n",
    "        rank = df_prov.iloc[i]['Rank']\n",
    "        diz_prov[territorio][ind].append(rank)\n",
    "\n",
    "    # MISSING VALUES\n",
    "    j = 1\n",
    "    missing_ter = set(provinces) - set(df_prov.TERRITORIO.to_list())\n",
    "    for mt in missing_ter:\n",
    "        #print(mt)\n",
    "        diz_prov[mt][ind].append(len(df_prov)+j)\n",
    "        j += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary for the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_reg = dict()\n",
    "for reg in Bes_Regions:\n",
    "    classes = os.listdir('D:\\Geospatial_Project\\Dati_Streamlite')\n",
    "    classes.remove('Wk_2')\n",
    "    classes.remove('Subjective well-being')\n",
    "    diz_ind = dict()\n",
    "    for folder in classes:\n",
    "        diz_ind[folder] = []\n",
    "                \n",
    "    diz_reg[reg] = diz_ind "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the rankings into the dictionary\n",
    "\n",
    "NB: This operation needs to be repeated for all the indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'D:\\Geospatial_Project\\Dati_Streamlite\\\\'\n",
    "ind = 'Economic well-being'\n",
    "for sub_ind in os.listdir(dir+ind):\n",
    "    # Read the geodataframe\n",
    "    Reg_df = gpd.read_file(Reg_Path)\n",
    "    df = read_dati_bes(dir+ind+ '\\\\' + sub_ind)\n",
    "    ## PREPARE REG GEO DF ## \n",
    "    Reg_df = mod_col_geo(Reg_df)\n",
    "    Reg_df = order_df_regions(Reg_df, Bes_Regions)\n",
    "    # Creating Regions DataFrame\n",
    "    df_reg = order_df(df, Bes_Regions)\n",
    "    # Create the geodaframe used in the representation\n",
    "    df_reg = from_df_to_gdf(df_reg, Reg_df)\n",
    "    # reset the index\n",
    "    df_reg.reset_index(inplace = True) \n",
    "    # check the data for the most recent year\n",
    "    for y_ in years: \n",
    "        if not df_reg[y_].isna().all():\n",
    "            break\n",
    "    \n",
    "    \n",
    "    df_reg = look_for_anomalies2(df_reg, y_)\n",
    "    if diz_sub[sub_ind]: # check whether having an high value is good or not \n",
    "        df_reg.sort_values(by = y_, inplace= True, ascending= False)\n",
    "    else:\n",
    "        df_reg.sort_values(by = y_, inplace = True)\n",
    "    \n",
    "    df_reg['Rank'] = list(range(1, len(df_reg)+1))\n",
    "\n",
    "    for i in range(len(df_reg)):\n",
    "        territorio = df_reg.iloc[i]['TERRITORIO']\n",
    "        rank = df_reg.iloc[i]['Rank']\n",
    "        diz_reg[territorio][ind].append(rank)\n",
    "\n",
    "    # MISSING VALUES\n",
    "    j = 1\n",
    "    missing_ter = set(Bes_Regions) - set(df_reg.TERRITORIO.to_list())\n",
    "    for mt in missing_ter:\n",
    "        diz_reg[mt][ind].append(len(df_reg)+j)\n",
    "        j += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macroareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_mac = dict()\n",
    "for mac in Macro_Areas:\n",
    "    classes = os.listdir('D:\\Geospatial_Project\\Dati_Streamlite')\n",
    "    classes.remove('Wk_2')\n",
    "    classes.remove('Subjective well-being')\n",
    "    diz_ind = dict()\n",
    "    for folder in classes:\n",
    "        diz_ind[folder] = []\n",
    "                \n",
    "    diz_mac[mac] = diz_ind "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the rankings into the dictionary\n",
    "\n",
    "NB: this operation needs to be repeated for all the indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'D:\\Geospatial_Project\\Dati_Streamlite\\\\'\n",
    "ind = 'Work & Life Balance'\n",
    "for sub_ind in os.listdir(dir+ind):\n",
    "    # Read the geodataframe\n",
    "    Macro_df = gpd.read_file(Macro_Path)\n",
    "    df = read_dati_bes(dir+ind+ '\\\\' + sub_ind) \n",
    "    ## PREPARE MACRO GEO DF ##\n",
    "    Macro_df = aggregate_macros(Macro_df)\n",
    "    # Creating Macro DataFrame\n",
    "    df_macro = order_df(df, Macro_Areas)\n",
    "    # Create the geodaframe used in the representation\n",
    "    df_macro = from_df_to_gdf(df_macro, Macro_df)\n",
    "    # reset the index\n",
    "    df_macro.reset_index(inplace = True) \n",
    "    # check the data for the most recent year\n",
    "    for y_ in years: \n",
    "        if not df_macro[y_].isna().all():\n",
    "            break\n",
    "    \n",
    "    \n",
    "    df_macro = look_for_anomalies2(df_macro, y_)\n",
    "    if diz_sub[sub_ind]: # check whether having an high value is good or not \n",
    "        df_macro.sort_values(by = y_, inplace= True, ascending= False)\n",
    "    else:\n",
    "        df_macro.sort_values(by = y_, inplace = True)\n",
    "    \n",
    "    df_macro['Rank'] = list(range(1, len(df_macro)+1))\n",
    "\n",
    "    for i in range(len(df_macro)):\n",
    "        territorio = df_macro.iloc[i]['TERRITORIO']\n",
    "        rank = df_macro.iloc[i]['Rank']\n",
    "        diz_mac[territorio][ind].append(rank)\n",
    "\n",
    "    # MISSING VALUES\n",
    "    j = 1\n",
    "    missing_ter = set(Macro_Areas) - set(df_macro.TERRITORIO.to_list())\n",
    "    for mt in missing_ter:\n",
    "        diz_mac[mt][ind].append(len(df_macro)+j)\n",
    "        j += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to skip the above computations, it is possible to save these three dictionaries via ´pickle´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_prov.pkl', 'wb') as f:\n",
    "    pickle.dump(diz_prov, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_reg.pkl', 'wb') as f:\n",
    "    pickle.dump(diz_reg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_mac.pkl', 'wb') as f:\n",
    "    pickle.dump(diz_mac, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_mac.pkl', 'rb') as f:\n",
    "    prova_mac = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_reg.pkl', 'rb') as f:\n",
    "    prova_reg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Geospatial_Project\\Dictionaries\\diz_prov.pkl', 'rb') as f:\n",
    "    prova_prov = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatori = os.listdir('D:\\Geospatial_Project\\Dati_Streamlite')\n",
    "indicatori.append('TERRITORIO')\n",
    "indicatori.remove('Subjective well-being')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_ranking(prova_mac, indicatori):\n",
    "    ranking = pd.DataFrame(columns=indicatori)\n",
    "    row = []\n",
    "    for territorio in prova_mac:\n",
    "        for ind in prova_mac[territorio]:\n",
    "            row.append(sum(prova_mac[territorio][ind]))\n",
    "\n",
    "        row.append(territorio)    \n",
    "        series = pd.Series(row, index = ranking.columns)\n",
    "        ranking = ranking.append(series, ignore_index= True)\n",
    "\n",
    "        row = []\n",
    "    ranking.set_index('TERRITORIO', inplace = True)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macroareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = df_ranking(prova_mac, indicatori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macro_df = gpd.read_file(Macro_Path)\n",
    "Macro_df = aggregate_macros(Macro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking.set_index('TERRITORIO', inplace = True)\n",
    "ranking = from_df_to_gdf(ranking, Macro_df)\n",
    "ranking.reset_index(inplace= True)\n",
    "ranking.sort_values(by = 'Environment', inplace = True) # we have chosen environment as example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create the map__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "j = 1 \n",
    "for index, row in ranking.iterrows():\n",
    "  icon=folium.features.CustomIcon('D:/Geospatial_Project/Medaglie/' + str(j) + '.png' ,icon_size=(40,40))\n",
    "  marker = getMarker(row['geometry'].centroid.y,row['geometry'].centroid.x, row['TERRITORIO'], icon)\n",
    "  marker.add_to(m5)\n",
    "  j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_df = mod_col_geo(Reg_df)\n",
    "Reg_df = order_df_regions(Reg_df, Bes_Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = df_ranking(prova_reg, indicatori)\n",
    "ranking = from_df_to_gdf(ranking, Reg_df)\n",
    "ranking.reset_index(inplace = True)\n",
    "ranking.sort_values(by = ['Health'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking2 = ranking.iloc[:3] #we care about the first three\n",
    "ranking2.to_crs(4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m6=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "j = 1 \n",
    "for index, row in ranking2.iterrows():\n",
    "  icon=folium.features.CustomIcon('D:/Geospatial_Project/Medaglie/' + str(j) + '.png' ,icon_size=(40,40))\n",
    "  marker = getMarker(row['geometry'].centroid.y,row['geometry'].centroid.x, row['TERRITORIO'], icon)\n",
    "  marker.add_to(m6)\n",
    "  j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = df_ranking(prova_prov, indicatori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prov_df = gpd.read_file(Prov_Path)\n",
    "Prov_df = clean_prov_geo(Prov_df, provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = from_df_to_gdf(ranking, Prov_df)\n",
    "ranking.reset_index(inplace= True)\n",
    "ranking.sort_values(by = 'Environment', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking2 = ranking.iloc[:3] #we care about the first three\n",
    "ranking2.to_crs(4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m7=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "j = 1 \n",
    "for index, row in ranking2.iterrows():\n",
    "  icon=folium.features.CustomIcon('D:/Geospatial_Project/Medaglie/' + str(j) + '.png' ,icon_size=(40,40))\n",
    "  marker = getMarker(row['geometry'].centroid.y,row['geometry'].centroid.x, row['TERRITORIO'], icon)\n",
    "  marker.add_to(m7)\n",
    "  j += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38951ef4700dfc932376dde0985e93a0bc93d263c8cdf2b090d6456cdc089de6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
