{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BES INDICATORS IN ITALY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [__Bes project__](https://www4.istat.it/en/well-being-and-sustainability/well-being-measures/bes-report) \n",
    "    was launched in 2010 to measure Equitable and Sustainable Well-being,\n",
    "    and with the aim of evaluating the progress of society not only from an economic,\n",
    "    but also from a social and environmental point of view. \n",
    "    \\\\\n",
    "    To this end, the traditional economic indicators, GDP first of all, have been integrated with measures of the quality of people’s life and of the environment.\n",
    "    \\\\\n",
    "    \\\\\n",
    "    Since 2016, well-being indicators and welfare analyzes have been presented with indicators for monitoring the objectives of the 2030 Agenda\n",
    "    for Sustainable Development,\n",
    "    the so-called [Sustainable Development Goals (SDGs)](https://sdgs.un.org/goals) of the United Nations.\n",
    "    They were chosen by the global community through a political agreement between the different actors, to represent their values, priorities and objectives.\n",
    "    The United Nations Statistical Commission (UNSC) has set up a shared set of statistical information to monitor the progress of individual countries towards the SDGs,\n",
    "    including over two hundred indicators.\n",
    "    The two sets of indicators are only partially overlapping, but certainly complementary.\n",
    "    \\\\\n",
    "    \\\\\n",
    "    Bes' indicators cover [12 domains relevant for the measuramente of the well-being](https://www.istat.it/it/files/2018/04/12-domains-scientific-commission.pdf) \n",
    "    and they are the following: \n",
    "\n",
    "1)  Health \n",
    "2)  Education & Training\n",
    "3)  Work & Life Balance\n",
    "4)  Economic well-being\n",
    "5)  Social Relationships\n",
    "6)  Politics & Istitutions\n",
    "7)  Security \n",
    "8)  Subjective well-being\n",
    "9)  Landscape & Cultural heritage\n",
    "10) Environment\n",
    "11) Innovation, Research & Creativity\n",
    "12) Quality of services\n",
    "\n",
    "These twleve indicators are themselves subdivided into smaller subindicators and each of them refer to a specific measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dowloaded from the national institute of statistics is organized in a huge excel. This file contains the data for all the indicators. \n",
    "<br>\n",
    "In order to analyze and represent the data there is the need to split this file in smaller files: One for each subindicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dependencies\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "dati_bes = pd.read_excel('E:\\Geospatial_Project\\Dati Bes\\Bes\\Indicatori_per_provincia_sesso.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "header = dati_bes.columns\n",
    "tabella_new = pd.DataFrame(columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data in an automatic manner there is the need to create an algorithm that detects whether there is a change in the unit of measure or in the indicator.\n",
    "<br>\n",
    "A smart way to do so is by looking at the territories. Indeed, there is a change in the subindicators everytime the data refer to ´Italy´ which is the average that for that specific measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern that change \n",
    "for j in range(len(dati_bes)):\n",
    "    mini_ds = dati_bes.iloc[j]\n",
    "    tabella_new = tabella_new.append(mini_ds)\n",
    "    if mini_ds['TERRITORIO'] == 'Italia':\n",
    "        Dominio = dati_bes.iloc[j]['DOMINIO']\n",
    "        Indicatore = dati_bes.iloc[j]['INDICATORE']\n",
    "        Sesso = dati_bes.iloc[j]['SESSO']\n",
    "        Unita = dati_bes.iloc[j]['UNITA_MISURA']\n",
    "        tabella_new.to_excel('E:/Geospatial_Project/Nuovi_Dati2/{Dominio}-{Indicatore}-{Sesso}-{Unita}.xlsx'.format(Dominio=Dominio, Indicatore=Indicatore, Sesso = Sesso, Unita = Unita))\n",
    "        tabella_new = pd.DataFrame(columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the data has been properly stored\n",
    "files_ = os.listdir(path='Nuovi_Dati2')\n",
    "files_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether all the datasets have the same lengths. If not there is the need to \n",
    "#understand why\n",
    "to_chekc = []\n",
    "for file in files_:\n",
    "    if file.endswith('.xlsx'):\n",
    "        excel_ = pd.read_excel('Nuovi_Dati2/'+file)\n",
    "        if len(excel_) != 135:\n",
    "            print(file + str(len(excel_)))\n",
    "            to_chekc.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some datasets whose lengths is sligthly different compared to others. \n",
    "<br>\n",
    "Some hypothesis can be made in order to justify these differences:\n",
    "* maybe the province does not send the required data and so it has not been considered;\n",
    "* the province did not exist when the data collection started (the reference year is 2004) \n",
    "<br>\n",
    "\n",
    "To understand the real cause there is the need to see which territories are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prova = pd.read_excel('Nuovi_Dati2/Benessere economico-Retribuzione media annua dei lavoratori dipendenti-Totale-euro.xlsx')\n",
    "s2 = df_prova.TERRITORIO.unique().tolist()\n",
    "path = 'Nuovi_Dati2/'\n",
    "Territori_Mancanti = {}\n",
    "for f in to_chekc:\n",
    "    df_check = pd.read_excel(path + f)\n",
    "    s1 =df_check.TERRITORIO.unique().tolist()\n",
    "    mis_val = set(s2) - set(s1) \n",
    "    Territori_Mancanti[f] = mis_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territori_Mancanti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell, it is possible to notice that all the missing territories refer to the region of Sardinia. Hence it is possible to believe that the 'missing territories' refer to territories that have changed their territorial administration or something like that.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data has been stored in smaller files, one for each subindicators, it is possible to start the analysis and the representation of the data. \n",
    "<br>\n",
    "Firstly, there is the need to decide how do we want to organize the analysis: Indeed, we have data of related to provinces, regions and macroareas (north, centre, south) all together. Thus, there is the need to split the data in three subsets, one for each level of analysis:\n",
    "*  Macrolevel : A comparison between North, Center, and South of Italy:\n",
    "    * North = Piemonte, Valle d'Aosta, Lombardia, Liguria, Trentino-Alto Adige, Veneto, Friuli-Venezia-Giulia, Emilia-Romagna\n",
    "    * Center: Toscana, Umbria, Marche, Lazio\n",
    "    * South: Abruzzo, Molise, Campania, Puglia, Basilicata, Calabria, Sicilia, Sardegna\n",
    "* Regional level: A comparison among the twenty italian regions\n",
    "* Province level : A comperison among the one hundred and seven italian provinces. \n",
    "<br>\n",
    "Since we will end up with many differnt datasets I choose to create functions that clean and manipulate the datasets in an automatic manner and according to the diffent needs. \n",
    "<br>\n",
    "To clarify we will start having ordinary pandas dataframes and the output will be a geopandas dataframe that will allow us to create geospatial representation and to perform some kind of data analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required dependencies\n",
    "import os \n",
    "import esda\n",
    "import math\n",
    "import folium\n",
    "import leafmap\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "import libpysal as lps\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dati_bes(path):\n",
    "    \n",
    "    '''\n",
    "    Through this function the excel file:\n",
    "    * is read and imported;\n",
    "    * is reduce by dropping unuseful columns     \n",
    "    '''\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    df.drop(columns = ['Unnamed: 0', 'DOMINIO', 'CODICE', 'SESSO', 'FONTE'], inplace = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "def provinces_BES(df, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions):\n",
    "\n",
    "    '''\n",
    "    Due to the difference in annotation between the BES_Indicator data and the ones of ISTAT,\n",
    "    there is the need to change some of the names of the provinces.\n",
    "    Firstly, we should brake the dataset of BES Indicators in order to separate regions and macro areas,\n",
    "    NB: We have to check that there are NO differences between regions'names\n",
    "    Secondly, we have to omologate the names of the two dataset.\n",
    "    NB: There are substantial differences mainly involving Sardinia, where ISTAT's Sud-Sardegnia contains BES'S Sud-Sardegna, Medio-Campidano, Carbonia-Iglesias, Ogliastra and Olbia-Tempio.\n",
    "    '''\n",
    "\n",
    "    #all_territories = df.TERRITORIO.unique().tolist()\n",
    "    #return set(all_territories) - set(Regions) - set(Anomalous_Regions) - set(Sud_Sardinia) - set(Macro_Areas) - set(Italy)\n",
    "    all_territories = df.TERRITORIO.unique().tolist()\n",
    "    territories =  set(all_territories) - set(Regions) - set(Anomalous_Regions) - set(Sud_Sardinia) - set(Macro_Areas) - set(Italy)\n",
    "    territories = list(territories)\n",
    "    territories.sort()\n",
    "    return territories\n",
    "\n",
    "def regions_BES(Regions):\n",
    "    '''\n",
    "    Due to the difference in annotation between the BES_Indicator data and the ones of ISTAT,\n",
    "    there is the need to change some of the names of the regions.\n",
    "    '''\n",
    "\n",
    "    new_regions = []\n",
    "    for reg in Regions:\n",
    "        if reg ==\"Valle d'Aosta\":\n",
    "            new_regions.append(\"Valle d'Aosta/Vallée d'Aoste\")\n",
    "        elif reg == 'Trentino-Alto Adige':\n",
    "            new_regions.append(\"Trentino-Alto Adige/Südtirol\")\n",
    "        elif reg == 'Friuli Venezia Giulia':\n",
    "            new_regions.append('Friuli-Venezia Giulia')\n",
    "        else:\n",
    "            new_regions.append(reg)\n",
    "    \n",
    "    return new_regions\n",
    "\n",
    "\n",
    "def mod_col_geo(geodf):\n",
    "    \n",
    "    '''\n",
    "    This function drops useless columns to the geodataframe and rename one of its column\n",
    "    '''\n",
    "    \n",
    "    geodf.drop(columns=['COD_RIP','COD_REG'], inplace = True)\n",
    "    geodf.rename(columns={'DEN_REG':'Reg'}, inplace = True) \n",
    "    \n",
    "    return geodf\n",
    "\n",
    "def get_regions_names(geodf):  \n",
    "\n",
    "    '''\n",
    "    This function returns a list of all the regions names according to the ISTAT standard\n",
    "    NB there are some differences among the names used by ISTAT and the ones used for the BES indicators \n",
    "    '''\n",
    "\n",
    "    return  geodf.Reg.unique().tolist() \n",
    "\n",
    "def sort_reset_index_geo(geo_df):\n",
    "\n",
    "    '''\n",
    "    The geodaframe is sorted and its indeces are reset\n",
    "    (This will be useful when the main dataframe will be converted into a geopandas dataframe)\n",
    "    '''\n",
    "    geo_df.sort_values(by = ['Prov'], inplace = True) \n",
    "    geo_df.reset_index(inplace = True)\n",
    "\n",
    "    return geo_df\n",
    "\n",
    "def order_df(df, BES_Territories):\n",
    "\n",
    "    '''\n",
    "    The daframe is:\n",
    "    * reduced in order to have coherence with the geodatafrmae which considers less provinces than the one of bes (actually, the geodatraframe respects the actual division of Italy into provinces)\n",
    "    * sorted\n",
    "    * its indeces are reset\n",
    "    (This will be useful when the main dataframe will be converted into a geopandas dataframe)\n",
    "    '''\n",
    "\n",
    "    prov_df =  df[df['TERRITORIO'].isin(BES_Territories)]\n",
    "\n",
    "    if len(prov_df) > 3:\n",
    "        prov_df.sort_values(by=['TERRITORIO'],inplace = True)\n",
    "        \n",
    "    \n",
    "    prov_df.set_index('TERRITORIO', inplace = True)\n",
    "\n",
    "    return prov_df\n",
    "\n",
    "\n",
    "def clean_prov_geo(geo_prov, provinces):\n",
    "\n",
    "    '''\n",
    "    This functions cleans the geodataframe relative to provinces. \n",
    "    What this function does is:\n",
    "    * dropsuseless columns\n",
    "    * sorts values according to territories' names\n",
    "    * sets the territories names as indexes\n",
    "    '''\n",
    "      \n",
    "    geo_prov.drop(columns = ['COD_RIP','COD_REG',\t'COD_PROV',\t'COD_CM',\t'COD_UTS',\t'DEN_PROV',\t'DEN_CM', 'TIPO_UTS'])\n",
    "    geo_prov.sort_values(by = 'DEN_UTS', inplace = True)\n",
    "    geo_prov.DEN_UTS = provinces\n",
    "    geo_prov.set_index('DEN_UTS', inplace = True)\n",
    "  \n",
    "    return geo_prov\n",
    "\n",
    "def order_df_regions(geodf, BES_Territories):\n",
    "    '''\n",
    "    This function modifies the geodaframe in order to have the same nomenclature for both the df and the geodf.\n",
    "    This step  is needed in order to merge the two at the next step.\n",
    "    '''\n",
    "    \n",
    "    geodf['Reg'] = BES_Territories #CHANGE\n",
    "    geodf.sort_values(by= ['Reg'], inplace = True)\n",
    "    geodf.set_index('Reg', inplace = True )\n",
    "    \n",
    "    return geodf\n",
    "\n",
    "def aggregate_macros(geodf):\n",
    "    \n",
    "    '''\n",
    "    The data regarding BES indicators refer to just three macroareas insted of 6 as in the ISTAT dataset. \n",
    "\n",
    "    Hence, there is the need to marge the areas in order have 3 main areas\n",
    "\n",
    "    1.   North : Piemonte, Valle d'Aosta/Vallée d'Aoste, Lombardia, Liguria, Trentino-Alto Adige/Südtirol, Veneto, Friuli-Venezia Giulia, Emilia-Romagna\n",
    "    2.   Centre: Toscana, Umbria, Marche, Lazio\n",
    "    3.   South(& Islands): Abruzzo, Molise, Campania, Puglia, Basilicata, Calabria, Sicilia, Sardegna  \n",
    "    '''\n",
    "\n",
    "    Territorio = ['Nord', 'Nord', 'Centro', 'Mezzogiorno', 'Mezzogiorno']\n",
    "    geodf['TERRITORIO'] = Territorio\n",
    "    geodf = geodf.to_crs(epsg=4326).dissolve(by='TERRITORIO')\n",
    "    geodf.drop(columns = ['DEN_RIP'])\n",
    "    geodf.sort_values(by = ['COD_RIP'], inplace = True)\n",
    "\n",
    "    return geodf\n",
    "\n",
    "def from_df_to_gdf(df, geo_df):\n",
    "\n",
    "    '''\n",
    "    With this function we convert the dataframe containing the statistics we are interested in into a geodaframe. \n",
    "    This allows us to use all the functionalities of a geopandas dataframe such us doing plots.\n",
    "    '''\n",
    "\n",
    "    df['Shape_Leng'] = geo_df['Shape_Leng']\n",
    "    df['Shape_Area'] = geo_df['Shape_Area']\n",
    "    df['geometry'] = geo_df['geometry']\n",
    "    df = gpd.GeoDataFrame(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the geodataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, we have data regarding the different subindicators. However, in order to perform geospatial analyses and create geospatial representation there is the need to have the data about the ´geometries´ of the territories we are dealing with. \n",
    "<br>\n",
    "Thus, the first thing to do is importing the geodataframes provided by the national institute of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where are stored the shape files\n",
    "Reg_Path = 'Limiti/Lim/Reg01012021/Reg01012021_WGS84.shp'\n",
    "Prov_Path = 'Limiti/Lim/ProvCM01012021/ProvCM01012021_WGS84.shp'\n",
    "Macro_Path = 'Limiti/Lim/RipGeo01012021/RipGeo01012021_WGS84.shp'\n",
    "# Read the shape files and create the geodataframes\n",
    "# One for each territorial division (Macroareas, Regions and provinces)\n",
    "Reg_df = gpd.read_file(Reg_Path)\n",
    "Prov_df = gpd.read_file(Prov_Path)\n",
    "Macro_df = gpd.read_file(Macro_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now are created some lists containing the names of problematic territories. \n",
    "These lists will be used to create the geodataframes. Indeed, having the right and common nomenclature is needed in order to merge the pandas dataframe with the geopandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalous Regions names\n",
    "Anomalous_Regions = ['Trentino-Alto Adige/Südtirol','Friuli-Venezia Giulia',\"Valle d'Aosta/Vallée d'Aoste\"]\n",
    "#Sud_Sardinia_Agglomeration\n",
    "Sud_Sardinia = ['Ogliastra','Olbia-Tempio','Medio Campidano','Carbonia-Iglesias']\n",
    "# Macro_Areas \n",
    "Macro_Areas = ['Centro', 'Mezzogiorno', 'Nord']\n",
    "# Italy \n",
    "Italy = ['Italia']\n",
    "# Different_Names\n",
    "Different_Names_BES = ['Bolzano/Bozen', 'Forlì-Cesena', 'Massa-Carrara', 'Reggio Calabria']\n",
    "Deffirent_Names_ISTAT = ['Bolzano', \"Forli'-Cesena\", 'Massa Carrara', 'Reggio di Calabria']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following chunk of code we create all the python objects needed to obtain the geodaframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the BES_Statistics Dataframe\n",
    "path_ = 'E:/Geospatial_Project/Nuovi_Dati2/'\n",
    "List_Statistics = os.listdir(path = path_)\n",
    "# Read a random file just to compute the provinces afterward\n",
    "df_ = read_dati_bes(path_ + '/' + List_Statistics[0])\n",
    "# Get the Istat regions \n",
    "Regions = Reg_df.DEN_REG.to_list()\n",
    "# Get  BES Regions\n",
    "Bes_Regions = regions_BES(Regions)\n",
    "# Extract the set of all Bes provinces\n",
    "provinces = provinces_BES(df_, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provinces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the geodataframe related to provinces. \n",
    "<br>\n",
    "To do so we choose a random datasets from the set of all the dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any datasets relative to the one of the subindicators\n",
    "stat = List_Statistics[0]\n",
    "df = read_dati_bes(path_+ '/' + stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE PROV GEO DF ## \n",
    "Prov_df = clean_prov_geo(Prov_df, provinces)\n",
    "# Create the provinces df\n",
    "df_prov = order_df(df, provinces)\n",
    "# Obtain the full geodataframe of provinces \n",
    "df_prov = from_df_to_gdf(df_prov, Prov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the datasets\n",
    "df_prov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done for provinces we will create our geodataframe relative to the regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk of code does not need to be executed if the one above has been executed\n",
    "\n",
    "# Import any datasets relative to the one of the subindicators\n",
    "stat = List_Statistics[0]\n",
    "df = read_dati_bes(path_+ '/' + stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE REG GEO DF ##\n",
    "Reg_df = mod_col_geo(Reg_df)\n",
    "Reg_df = order_df_regions(Reg_df, Bes_Regions)\n",
    "# Creating Regions DataFrame\n",
    "df_reg = order_df(df, Bes_Regions)\n",
    "# Create the geodaframe used in the representation\n",
    "df_reg = from_df_to_gdf(df_reg, Reg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macroareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will create the macroareas geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE MACRO GEO DF ##\n",
    "Macro_df = aggregate_macros(Macro_df)\n",
    "# 'MACRO-AREA' #\n",
    "# Creating the MacroArea Dataframe\n",
    "df_macro = order_df(df, Macro_Areas)    \n",
    "# Create the geodaframe used in the representation\n",
    "df_macro = from_df_to_gdf(df_macro, Macro_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all the geodataframes, it is possible to create some representation. \n",
    "Two kind of geospatial plots will be created:\n",
    "* A static choropletmap via Matplotlib that will show the data (if specified) groupd in quantiles\n",
    "* An interactive choropletmap created with folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As did before, to avoid long and repetitive chunks of code will be defined some functions at the top that will be then called to create the needed plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_(file_name):\n",
    "    \n",
    "    '''\n",
    "    This function provides the title to plots \n",
    "    '''\n",
    "\n",
    "    title = file_name.strip('.xlsx')\n",
    "    title = title.split('-')\n",
    "    title = title[0].upper() + ' ' + title[1] + ' ' + title[2]\n",
    "\n",
    "    return title \n",
    "\n",
    "def get_labels_(df):\n",
    "    \n",
    "    '''\n",
    "    This functions returns the indicators that will show up in the folium map while using the pointer\n",
    "    over the territories involved in the analysis\n",
    "    '''\n",
    "    \n",
    "    return df.iloc[0]['INDICATORE'] + '\\n' + '(' + df.iloc[0]['UNITA_MISURA'] + ')'\n",
    "\n",
    "def static_choroplet(df, variable, title_, scheme = None, K =None,  color_map = None):\n",
    "\n",
    "    '''\n",
    "    With this function is it possible to create a static Choroplet map \n",
    "    Additional feature that can be added are ad-hoc colormaps, and a scheme for the division of the classes.\n",
    "    '''\n",
    "    \n",
    "    miss_df = df[df[variable].isna()]\n",
    "    df.dropna(subset = [variable], inplace = True)\n",
    "    ax = df.plot(column= variable, \n",
    "    legend=True,\n",
    "    figsize=(14,14),\n",
    "    edgecolor=\"lightgray\", \n",
    "    linewidth = 0.9,\n",
    "    scheme = scheme,\n",
    "    cmap = color_map,\n",
    "    k = K)\n",
    "    if len(miss_df) > 0 : \n",
    "        miss_df.plot(color = 'gray', ax = ax)\n",
    "    ax.set_title(title_)\n",
    "    ax.set_axis_off()\n",
    "    #plt.show()\n",
    "    fig = ax.figure\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def dynamic_choroplet(df, title_, measure):\n",
    "    \n",
    "    '''\n",
    "    With this function it is possible to create an interactive choropletmap\n",
    "    '''\n",
    "\n",
    "    fig = px.choropleth(df,\n",
    "                   geojson=df.geometry,\n",
    "                   locations=df.index, # maybe this should be changed for provinces \n",
    "                   color=measure,\n",
    "                    title = title_)\n",
    "    fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    #fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def folium_interactive_map(df, var, file_name, indicator):\n",
    "    \n",
    "    '''\n",
    "    This function will create an interactive folium choropletmap. Moreover, this maps provides also a feature\n",
    "    that allows to see the specifi measure when the reader uses the pointer over the territories involved\n",
    "    in the analysis\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # Create the folium map \n",
    "    m10=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=5)\n",
    "\n",
    "    df = look_for_anomalies2(df, var)\n",
    "    df.to_crs(4326, inplace = True)\n",
    "\n",
    "    \n",
    "    folium.Choropleth(\n",
    "    geo_data = df.to_json(),\n",
    "    data = df,\n",
    "    columns=['TERRITORIO', var],\n",
    "    key_on='feature.properties.TERRITORIO',\n",
    "    #key_on = 'feature.properties.id',\n",
    "    fill_color='Oranges', \n",
    "    fill_opacity=0.6, \n",
    "    line_opacity=1,\n",
    "    nan_fill_color='black',\n",
    "    legend_name= get_title_(file_name),\n",
    "    smooth_factor=0).add_to(m10)\n",
    "\n",
    "\n",
    "    \n",
    "    #add the feature\n",
    "    folium.features.GeoJson(df,\n",
    "                        name='Labels',\n",
    "                        style_function=lambda x: {'color':'transparent','fillColor':'transparent','weight':0},\n",
    "                        tooltip=folium.features.GeoJsonTooltip(fields=[var],\n",
    "                                                                aliases = [indicator], #Substitute with the indicator of the df\n",
    "                                                                labels=True,\n",
    "                                                                sticky=False\n",
    "                                                                            )\n",
    "                       ).add_to(m10)\n",
    "    \n",
    "\n",
    "    return m10\n",
    "\n",
    "def look_for_anomalies2(df,var):\n",
    "\n",
    "    '''\n",
    "    This functions is foudnamental to properly create the geospatial repersentations. \n",
    "    Indeed, in the original file dowloaded from the national istitute of statistics, missing values are treated \n",
    "    in different ways: some missing values are reported as a string like that '...', \n",
    "    others are in the format np.nan and lastly some are in the formar math.nan.\n",
    "    This functions simply drops the missing values for the specific year of analysis. \n",
    "    '''\n",
    "\n",
    "    i = 0\n",
    "    in_set = set()\n",
    "\n",
    "    for el in df[var].values:\n",
    "        if type(el) == str or math.isnan(el): \n",
    "            \n",
    "            in_set.add(i)\n",
    "    \n",
    "        i += 1 \n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(list(in_set), inplace = True)   \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not it is possible to create a plot relative to a specific subindicators in a given year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a year in which we are interested to know about\n",
    "y_ = 'V_2016'\n",
    "# Create the static choropletmat for each of the geodaframe\n",
    "matplotlib_fig = static_choroplet(df_macro, variable = y_, title_ = titolo, color_map = 'viridis', scheme = 'quantiles', K = 5)\n",
    "matplotlib_fig2 = static_choroplet(df_reg, variable = y_, title_ = titolo, color_map = 'viridis', scheme = 'quantiles', K = 5)\n",
    "matplotlib_fig3 = static_choroplet(df_prov, variable = y_, title_ = titolo, color_map = 'viridis', scheme = 'quantiles', K = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " matplotlib_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folium interactive choroplet maps \n",
    "folium_interactive_map(df_macro, y_, stat, labels)\n",
    "folium_interactive_map(df_macro, y_, stat, labels)\n",
    "folium_interactive_map(df_macro, y_, stat, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling one of the above functions will display the folium map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above representation show the data for a specific measure in a fixed period of time, it can be useful to create an additional plot, a line chart, that helps to understand the trend of that specific measure. \n",
    "<BR>\n",
    "To do so we will use plotly, a python library that creates interactive plots. \n",
    "Indeed, this kind of chart allows to visualize the data of a specific territory by clicking on the territories in the right slider menù. Depending on the different level of analysis it is possible to decide which observation has to be hided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_chart_plotly(Bes_df, to_hide_1, to_hide_2, to_hide_3, titolo = ''):\n",
    "    \n",
    "    '''\n",
    "    This functions create a dataframe related to a specific measure where each observation refers to a \n",
    "    differnet year. \n",
    "    Then uses this daatframe to create a linechart. \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df_plotly = pd.DataFrame(columns = ['TERRITORIO', 'MISURA', 'ANNO'])\n",
    "    #if len(df_plotly) > 1:\n",
    "    territorio = []\n",
    "    misura = []\n",
    "    _anno_ = []\n",
    "    for i in range(len(Bes_df)):\n",
    "        for j in range(4,20):\n",
    "                \n",
    "            if j < 10:\n",
    "                anno = 'V_200' + str(j)\n",
    "            else:\n",
    "                anno = 'V_20' + str(j)\n",
    "                    \n",
    "            anno_ = int(anno[2:])\n",
    "            territorio.append(Bes_df.iloc[i]['TERRITORIO'])\n",
    "            misura.append(Bes_df.iloc[i][anno])\n",
    "            _anno_.append(anno_)\n",
    "\n",
    "        \n",
    "        \n",
    "    df_plotly['TERRITORIO'] = territorio\n",
    "    df_plotly['ANNO'] = _anno_\n",
    "    df_plotly['MISURA'] = misura\n",
    "\n",
    "\n",
    "    to_hide_1.extend(to_hide_2)\n",
    "    to_hide_1.extend(to_hide_3)\n",
    "    fig = px.line(df_plotly, x = 'ANNO', y = 'MISURA', color = 'TERRITORIO', title = titolo)\n",
    "    fig.for_each_trace(lambda trace: trace.update(visible=\"legendonly\") \n",
    "            if trace.name in to_hide_1 else ())\n",
    "        \n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Macros\n",
    "line_chart_plotly(df, to_hide_1 = provinces, to_hide_2 = Bes_Regions, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regions\n",
    "line_chart_plotly(df, to_hide_1= provinces, to_hide_2 = Macro_Areas, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provinces\n",
    "line_chart_plotly(df, to_hide_1 = Bes_Regions, to_hide_2 = Macro_Areas, to_hide_3 = Sud_Sardinia, titolo = titolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the above choroplets maps, it is possible to create some additional representation that can support the above maps and maybe help to better understand/interpret the data.\n",
    "For this purpose, data from Open street Map will be used to retrieve geodata. For instance, it is possible to collect the data about all the factories in Italy in order to understand if there is some correlation between the presence of many facotries and some indexes related to the enviroment (in the context of pollution) or to the economic wellbeing (many factories are connected to more employment?!). \n",
    "<br>\n",
    "The starting point is dowloading the pbf files of the different regions and or provinces from [Wikimedia Italia](https://osmit-estratti.wmcloud.org/)\n",
    "<br>\n",
    "Secondly there is the data extraction via the osm library called pyrosm. \n",
    "<br>\n",
    "Here it is important to specify two things if someone wants to execut the chunks of code in this seciton of the notebook:\n",
    "* Since some Regions' PBF files were too big, it was infeasible for my local machine to run the code, hence I needed to download the PBFs files of all the provices belonging to these regions. \n",
    "* To run this code it is necessary to have a virtual environment where pyrosm is installed. To do so I created a new envrinoment where I have installed as first package pyrosm. Geopandas was automatcally installed since it is one of the dependencies of pyrosm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pyrosm\n",
    "import pandas as pd \n",
    "import  geopandas as gpd\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above chunk of code it is possible to extract points related to many different locations based on a filter that specifies in which kind of places we are interested in and save them into shp or csv files. \n",
    "The tags system is defined by OSM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the one where are saved all the PBFs files dowloaded from Wikimedia Italia\n",
    "os.chdir('E:/Geospatial_Project/Big_Cities')\n",
    "# Create a list with all the pbfs files \n",
    "regions_pbf = os.listdir()\n",
    "custom_filter = {'amenity' : ['srts_centre', 'brothel', 'casino', 'cinema',\n",
    "                            'community_centre', 'conference_centre', 'events_venue', 'fountain',\n",
    "                            'gambling', 'love_hotel', 'nightclub', 'planetarium', 'public_bookcase',\n",
    "                            'social_centre', 'stripclub', 'studio', 'swingerclub', 'theatre']}\n",
    "# Define the filter\n",
    "\n",
    "\n",
    "# Iterate along all the regions (or provinces)\n",
    "for region in regions_pbf:\n",
    "    \n",
    "    name = region.split('_')\n",
    "    name = name[1]\n",
    "    \n",
    "    osm = pyrosm.OSM(region)\n",
    "    data = osm.get_pois(custom_filter = custom_filter)\n",
    "    \n",
    "    try:\n",
    "        data.to_file('E:/Geospatial_Project/Freetime/' + name + '/' + name + '.shp')\n",
    "    except:\n",
    "        data = pd.DataFrame(data)\n",
    "        data.to_csv('E:/Geospatial_Project/Freetime/' + name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_shp(path):\n",
    "\n",
    "    '''\n",
    "    This function read the csv and shp files that contain the geodata retrived from osm.\n",
    "    Then it merges all this files into a big datasets where are stored the latitude, longitude \n",
    "    and the name of the point of interest.    \n",
    "    '''\n",
    "    \n",
    "    _full_df_ = gpd.GeoDataFrame(crs = 4326, columns = ['name', 'lat', 'lon'])\n",
    "    lats = []\n",
    "    lons = []\n",
    "    names = []\n",
    "    list_files = os.listdir(path)\n",
    "    list_files_ = []\n",
    "\n",
    "\n",
    "    for file_ in list_files:\n",
    "        if file_.endswith('.csv'):\n",
    "            list_files_.append(file_)\n",
    "        elif file_.endswith('.shp'):\n",
    "            list_files_.append(file_)\n",
    "\n",
    "\n",
    "    for _file_ in list_files_:\n",
    "        \n",
    "        if _file_.endswith('.csv'):\n",
    "            _df_ = pd.read_csv(path + '/' + _file_)\n",
    "            if len(_df_) > 0:\n",
    "                _df_['geometry'] = gpd.GeoSeries.from_wkt(_df_['geometry'])\n",
    "                _geodf_ = gpd.GeoDataFrame(_df_, crs = 4326)\n",
    "        \n",
    "        elif _file_.endswith('.shp'):\n",
    "            _geodf_ = gpd.read_file(path + '/' + _file_)\n",
    "        lats.extend(_geodf_.geometry.centroid.to_crs(4326).y)\n",
    "        lons.extend(_geodf_.geometry.centroid.to_crs(4326).x)\n",
    "\n",
    "        if 'name' in _geodf_.columns:\n",
    "            names.extend(_geodf_.name)\n",
    "        else: \n",
    "            names.extend(['Missing'] * len(_geodf_.geometry.centroid.to_crs(4326).x))\n",
    "    \n",
    "\n",
    "    new_names = []\n",
    "    for n in names: \n",
    "        if type(n) != str:\n",
    "            new_names.append('Missing')\n",
    "        else:\n",
    "            new_names.append(n)\n",
    "    \n",
    "\n",
    "    _full_df_['lat'] = lats\n",
    "    _full_df_['lon']= lons\n",
    "    _full_df_['name'] = new_names\n",
    "\n",
    "    return _full_df_\n",
    "\n",
    "def getMarker(lat,lon, message,inconstyle):\n",
    "\n",
    "'''\n",
    "This function simply add a marker on a folium map based on the point latitude and logitude. \n",
    "In addition, there is also a pop up message that shows up when the user interact with the map. \n",
    "Lastly, it is even possible to specify an icon that defines the point on a map. \n",
    "'''\n",
    "\n",
    "    marker = folium.Marker(location=[lat,lon],\n",
    "                         popup=message,\n",
    "                         icon=inconstyle)\n",
    "    return marker\n",
    "\n",
    "\n",
    "def marker_plot(df, logo : str):\n",
    "    '''\n",
    "    This functions create a map. \n",
    "    Then it adds to this maps all the points present in the given datasets. \n",
    "    By iterating over the rwos of the provided dataset applies the function defined above \n",
    "    (getMarker) to add the marker. \n",
    "    '''\n",
    "\n",
    "    m5=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    for index, row in df.iterrows():\n",
    "        #icon=folium.Icon(color='purple',prefix='fa',icon='arrow-circle-down')\n",
    "        icon=folium.features.CustomIcon(logo, icon_size=(34,34))\n",
    "        marker = getMarker(row['lat'],row['lon'],row['name'], icon)\n",
    "\n",
    "        marker.add_to(m5)\n",
    "    \n",
    "    return  m5\n",
    "\n",
    "def marker_cluster(df, logo:str):\n",
    "\n",
    "'''\n",
    "This function creates a map where all the points are added, in the same way of the function marker plot,\n",
    "but it also aggregate into cluster closer points. This makes the map more good looking. \n",
    "Moreover, by zooming in into the map the cluser become smaller and smaller until it the cluster is made by\n",
    "just one point.  \n",
    "'''\n",
    "\n",
    "    m6=folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    marker_cluster = MarkerCluster().add_to(m6)\n",
    "    for index, row in df.iterrows():\n",
    "        #icon=folium.Icon(color='purple',prefix='fa',icon='arrow-circle-down')\n",
    "        icon=folium.features.CustomIcon(logo, icon_size=(34,34))\n",
    "        message = '<strong>sezione:'+ str(row['name'])\n",
    "        #tip = message + '<br/>' + row['via']\n",
    "        marker = getMarker(row['lat'],row['lon'],message, icon)\n",
    "        #add to marker cluster \n",
    "        marker.add_to(marker_cluster)\n",
    "    \n",
    "    \n",
    "    return m6\n",
    "\n",
    "def heatmap_plot(df):\n",
    "\n",
    "    '''\n",
    "    This function creates a map where we can visualize the distribution of points via a \n",
    "    heatmap. Indeed the more the color is closer to red the higher is the concetration of points\n",
    "    in that area.  \n",
    "    '''\n",
    "\n",
    "    m7 = folium.Map(location=[41.9027835,12.4963655],tiles='openstreetmap',zoom_start=6)\n",
    "    data = df[['lat','lon']]\n",
    "    HeatMap(data.values).add_to(m7)\n",
    "\n",
    "\n",
    "    return m7\n",
    "\n",
    "def title_folium_map(map, title:str):\n",
    "\n",
    "    '''\n",
    "    This function simply adds  the title to a folium map\n",
    "    '''\n",
    "\n",
    "    title = ''\n",
    "    title_html = '''\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{}</b></h3>\n",
    "             '''.format(title)   \n",
    "\n",
    "    map = folium.Map(location=[27.783889, -97.510556],\n",
    "               zoom_start=12)\n",
    "\n",
    "    map.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "    return map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, in the following chunk of code two maps concerning the italian factories in the italian territory are created: \n",
    "* the first is a map where factories are aggregated into clusters,\n",
    "* the second is an heatmap showing the distribution of factories along the italian territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = read_csv_shp('E:\\Geospatial_Project\\Factories')\n",
    "marker_cluster(_df_, logo = 'E:/Geospatial_Project/logo2.png')\n",
    "heatmap_plot(_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same as before but for No-profit organizations in Italy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_ = read_csv_shp('E:/Geospatial_Project/No_Profit')\n",
    "marker_cluster(_df_, logo = 'E:/Geospatial_Project/np2.jpg')\n",
    "heatmap_plot(_df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook we introduce methods of exploratory spatial data analysis that are intended to complement geovizualization through formal univariate and multivariate statistical tests for spatial clustering.\n",
    "<br>\n",
    "More specifically, there will be shown the results relatively to the networth pro capite. \n",
    "<br>\n",
    "NB: This measure has been chosen because ther is a high correaltion based on the loction of the territory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataframe related to the Networth pro  capite\n",
    "df = read_dati_bes('Benessere economico-Patrimonio pro capite-Totale-euro.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define both the geodataframe at regional and provincial level. \n",
    "Then we will merge these two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the provinces\n",
    "provinces = provinces_BES(df, Anomalous_Regions, Sud_Sardinia, Macro_Areas, Italy, Regions)\n",
    "## PREPARE PROV GEO DF ## \n",
    "Prov_df = clean_prov_geo(Prov_df, provinces)\n",
    "# Create the provinces df\n",
    "df_prov = order_df(df, provinces)\n",
    "# Obtain the full geodataframe of provinces \n",
    "df_prov = from_df_to_gdf(df_prov, Prov_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Istat regions \n",
    "Regions = Reg_df.DEN_REG.to_list()\n",
    "# Get  BES Regions\n",
    "Bes_Regions = regions_BES(Regions)\n",
    "# Extract the set of all Bes provinces\n",
    "## PREPARE REG GEO DF ##\n",
    "Reg_df = mod_col_geo(Reg_df)\n",
    "Reg_df = order_df_regions(Reg_df, Bes_Regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the crs in order to properly plot the geometries. \n",
    "df_prov.to_crs(4326, inplace = True)\n",
    "Reg_df.to_crs(4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the two dataframes.\n",
    "prova_df = gpd.sjoin(Reg_df, \n",
    "                          df_prov, how='inner', predicate='contains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova_df2 = prova_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average value by grouping provinces by their region (in 2017)\n",
    "median_val = prova_df2['V_2017'].groupby([prova_df2['Reg']]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the mean value of the networth pro-capite in the different regions. \n",
    "median_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new aggregated dataframe\n",
    "prova_df2 = prova_df.merge(median_val, on = 'Reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a simple choroplet map in order to have a first visualization of the data.\n",
    "prova_df2.plot(column = 'V_2017_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reate a more sophisticated choroplet maps \n",
    "#that divides the region (based on the networth pro-capite in 2017) in 5 groups.\n",
    "fig, ax = plt.subplots(figsize=(12,10), subplot_kw={'aspect':'equal'})\n",
    "prova_df2.plot(column='V_2017_y', scheme='Quantiles', k=5, cmap='GnBu', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spatial Autocorrelation:__\n",
    "\n",
    "Visual inspection of the map pattern for the pro-capite networth allows us to search for spatial structure. If the spatial distribution of the networth was random, then we should not see any clustering of similar values on the map. However, our visual system is drawn to the lighter clusters in the south as well as the center, and a concentration of the darker hues (higher networth pro capite) in the north. \n",
    "\n",
    "Our brains are very powerful pattern recognition machines. However, sometimes they can be too powerful and lead us to detect false positives, or patterns where there are no statistical patterns. This is a particular concern when dealing with visualization of irregular polygons of differning sizes and shapes ([Rey,2022](https://sergerey.org/)) ([Wolf, 2022](https://www.ljwolf.org/posts/2020-sm2-lectures/)).\n",
    "\n",
    "The concept of _spatial autocorrelation_ relates to the combination of two types of similarity: spatial similarity and attribute similarity. Although there are many different measures of spatial autocorrelation, they all combine these two types of simmilarity into a summary measure.\n",
    "\n",
    "Let's use [PySAL](https://pysal.org/) to generate these two types of similarity measures.\n",
    "\n",
    "__Spatial Similarity__\n",
    "In spatial autocorrelation analysis, the spatial weights are used to formalize the notion of spatial similarity. As we have seen there are many ways to define spatial weights, here we will use queen contiguity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prova_df2\n",
    "wq =  lps.weights.Queen.from_dataframe(df)\n",
    "wq.transform = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Similarity\n",
    "So the spatial weight between neighborhoods i and j indicates if the two are neighbors (i.e., geographically similar). What we also need is a measure of attribute similarity to pair up with this concept of spatial similarity. The spatial lag is a derived variable that accomplishes this for us. For neighborhood i the spatial lag is defined as:\n",
    "<br>\n",
    "                                   <h3><center>$ylag_{i}=\\sum_j{w_{i,j}y_{j}}$</center></h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['V_2017_y']\n",
    "ylag = lps.weights.lag_spatial(wq, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapclassify as mc\n",
    "ylagq5 = mc.Quantiles(ylag, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=ylagq5.yb).plot(column='cl', categorical=True, \\\n",
    "        k=5, cmap='GnBu', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.title(\"Spatial Lag Median Networth pro-capite (Quintiles)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quintile map for the spatial lag tends to enhance the impression of value similarity in space. It is, in effect, a local smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lag_median_val'] = ylag\n",
    "f,ax = plt.subplots(1,2,figsize=(2.16*4,4))\n",
    "df.plot(column='V_2017_y', ax=ax[0], edgecolor='k',\n",
    "        scheme=\"quantiles\",  k=5, cmap='GnBu')\n",
    "ax[0].axis(df.total_bounds[np.asarray([0,2,1,3])])\n",
    "ax[0].set_title(\"Networth pro-capite\")\n",
    "df.plot(column='lag_median_val', ax=ax[1], edgecolor='k',\n",
    "        scheme='quantiles', cmap='GnBu', k=5)\n",
    "ax[1].axis(df.total_bounds[np.asarray([0,2,1,3])])\n",
    "ax[1].set_title(\"Spatial Lag Neworth pro-capite\")\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we still have the challenge of visually associating the value of the networth in a region with the value of the spatial lag of values for the focal unit. The latter is a weighted average of list networths in the focal region.\n",
    "\n",
    "To complement the geovisualization of these associations we can turn to formal statistical measures of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Global Spatial Autocorrelation__\n",
    "We begin with a simple case where the variable under consideration is binary. This is useful to unpack the logic of spatial autocorrelation tests. So even though our attribute is a continuously valued one, we will convert it to a binary case to illustrate the key concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y > y.median()\n",
    "sum(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = y > y.median()\n",
    "labels = [\"0 Low\", \"1 High\"]\n",
    "yb = [labels[i] for i in 1*yb] \n",
    "df['yb'] = yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10), subplot_kw={'aspect':'equal'})\n",
    "df.plot(column='yb', cmap='binary', edgecolor='grey', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Join counts__\n",
    "One way to formalize a test for spatial autocorrelation in a binary attribute is to consider the so-called _joins_. A join exists for each neighbor pair of observations, and the joins are reflected in our binary spatial weights object wq.\n",
    "\n",
    "Each unit can take on one of two values \"Black\" or \"White\", and so for a given pair of neighboring locations there are three different types of joins that can arise:\n",
    "\n",
    "Black Black (BB)\n",
    "White White (WW)\n",
    "Black White (or White Black) (BW)\n",
    "Given that we have 47 Black polygons on our map, what is the number of Black Black (BB) joins we could expect if the process were such that the Black polygons were randomly assigned on the map? This is the logic of join count statistics.\n",
    "\n",
    "We can use the ´esda´ package from ´PySAL´ to carry out join count analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esda \n",
    "yb = 1 * (y > y.median()) # convert back to binary\n",
    "wq =  lps.weights.Queen.from_dataframe(df)\n",
    "wq.transform = 'b'\n",
    "np.random.seed(12345)\n",
    "jc = esda.join_counts.Join_Counts(yb, wq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results of the join analysis\n",
    "import seaborn as sbn\n",
    "sbn.kdeplot(jc.sim_bb, shade=True)\n",
    "plt.vlines(jc.bb, 0, 0.075, color='r')\n",
    "plt.vlines(jc.mean_bb, 0,0.075)\n",
    "plt.xlabel('BB Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density portrays the distribution of the BB counts, with the black vertical line indicating the mean BB count from the synthetic realizations and the red line the observed BB count for our prices. Clearly our observed value is extremely high. A pseudo p-value summarizes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the pseudo p-value\n",
    "jc.p_sim_bb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in networth pro-capite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Continuous Case__\n",
    "The join count analysis is based on a binary attribute, which can cover many interesting empirical applications where one is interested in presence and absence type phenomena. In our case, we artificially created the binary variable, and in the process we throw away a lot of information in our originally continuous attribute. Turning back to the original variable, we can explore other tests for spatial autocorrelation for the continuous case.\n",
    "\n",
    "First, we transform our weights to be row-standardized, from the current binary state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wq.transform = 'r'\n",
    "y = df['V_2017_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moran's I is a test for global autocorrelation for a continuous attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "mi = esda.moran.Moran(y, wq)\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our value for the statistic needs to be interpreted against a reference distribution under the null of CSR. PySAL uses a similar approach as we saw in the join count analysis: random spatial permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn.kdeplot(mi.sim, shade=True)\n",
    "plt.vlines(mi.I, 0, 1, color='r')\n",
    "plt.vlines(mi.EI, 0,1)\n",
    "plt.xlabel(\"Moran's I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Local Autocorrelation: Hot Spots, Cold Spots, and Spatial Outliers__\n",
    "In addition to the Global autocorrelation statistics, PySAL has many local autocorrelation statistics. Let's compute a local Moran statistic for the same d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq.transform = 'r'\n",
    "lag_price = lps.weights.lag_spatial(wq, df['V_2017_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df['V_2017_y']\n",
    "b, a = np.polyfit(price, lag_price, 1)\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "\n",
    "plt.plot(price, lag_price, '.', color='firebrick')\n",
    "\n",
    " # dashed vert at mean of the price\n",
    "plt.vlines(price.mean(), lag_price.min(), lag_price.max(), linestyle='--')\n",
    " # dashed horizontal at mean of lagged price \n",
    "plt.hlines(lag_price.mean(), price.min(), price.max(), linestyle='--')\n",
    "\n",
    "# red line of best fit using global I as slope\n",
    "plt.plot(price, a + b*price, 'r')\n",
    "plt.title('Moran Scatterplot')\n",
    "plt.ylabel('Spatial Lag of networth pro-capite')\n",
    "plt.xlabel('Networth pro-capite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of a single I statistic, we have an array of local Ii statistics, stored in the ´.Is´ attribute, and p-values from the simulation are ´in p_sim´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = esda.moran.Moran_Local(y, wq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can distinguish the specific type of local spatial association reflected in the four quadrants of the Moran Scatterplot above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = li.p_sim < 0.05\n",
    "hotspot = sig * li.q==1\n",
    "coldspot = sig * li.q==3\n",
    "doughnut = sig * li.q==2\n",
    "diamond = sig * li.q==4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'hot spot']\n",
    "labels = [spots[i] for i in hotspot*1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['red', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'cold spot']\n",
    "labels = [spots[i] for i in coldspot*1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['blue', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 1 * (li.p_sim < 0.05)\n",
    "hotspot = 1 * (sig * li.q==1)\n",
    "coldspot = 3 * (sig * li.q==3)\n",
    "doughnut = 2 * (sig * li.q==2)\n",
    "diamond = 4 * (sig * li.q==4)\n",
    "spots = hotspot + coldspot + doughnut + diamond\n",
    "spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_labels = [ '0 ns', '1 hot spot', '2 doughnut', '3 cold spot', '4 diamond']\n",
    "labels = [spot_labels[i] for i in spots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap([ 'lightgrey', 'red', 'lightblue', 'blue', 'pink'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "df.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMMENT RESULTS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isochrone map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have many data for each indicator, the data related to the _Subjective wellbeing_ are totally missing. \n",
    "<br>\n",
    "According to the [BES](https://www.istat.it/it/files/2018/04/12-domains-scientific-commission.pdf) this indicator relies on four sub-indicators:\n",
    "1) Life satisfaction\n",
    "2) Leaisure time satisfaction \n",
    "3) Positive judgments for future perspective\n",
    "4) Negative judgments for future perspecitve\n",
    "<br>\n",
    "\n",
    "\n",
    "It is quite difficult to find this kind of data without having the results of ad-hoc surveys constructed for this specific aim. Moreover, it is also difficult to find some other index/indicators that can approximate them especially if we work with geodata. \n",
    "<br>\n",
    "\n",
    "However, could be interesting to see how many amenties can be reached in 10-20minutes (isochrone maps) from the city center. Indeed, having this kind of representation could help us to understand something about the second sub-indicator: Leisure time satisfaction. As a matter of fact, having many pubs, cinema and other places where it is possible to spend your free time can be positively correlated to have a high leisure-time satisfaction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies \n",
    "import openrouteservice as ors\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chunk of code we take advantage of the geocoder to extract the representative points of the 10 biggest italian cities.\n",
    "<br>\n",
    "NB: we consider just the first 10 cities to obtain a clearer map. For instance, if we decided to collect the amenities for all the provinces of italy, then the map would have been too messy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['city']\n",
    "names = [('Roma'),('Torino'),('Genova'),('Bari'), ('Milano'), ('Genova'), ('Palermo'), ('Napoli'), ('Bologna'), ('Verona')]\n",
    "cities = gpd.GeoDataFrame(names,columns=cols)\n",
    "geo_cities = gpd.tools.geocode(cities.city, provider=\"arcgis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As did before, we use osm to extract the location of all the amanities related to freetime activities and entratainment in the required places. \n",
    "Then through the already mentioned function ´read_csv_shp´ we create a dataframe with the coordinates and names of these facilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = read_csv_shp('Freetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we firstly create a folium marker cluster map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = marker_cluster(complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we had the isochrone maps that highlight the facilities that can be reached in 10-20minutes of walk having as starting point the representative point of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ors_key = '5b3ce3597851110001cf6248d3c5776fac4c4918b5258bb3659ddec6'\n",
    "client = ors.Client(key=ors_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(geo_cities)):\n",
    "  city = geo_cities.iloc[i]\n",
    "  coordinate = [[city.geometry.centroid.x, city.geometry.centroid.y]]\n",
    "  iscorhrone = client.isochrones( locations=coordinate,\n",
    "    profile='foot-walking',\n",
    "    range=[600, 1200],\n",
    "    validate=False,)\n",
    "  popup_message = f'outline shows areas reachable within 10-20 minutes'\n",
    "  folium.GeoJson(iscorhrone, name='isochrone', tooltip= popup_message).add_to(m5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43ea1395c23b0bf42b5cffce1f3b376fef405030d45de64227aff00f2d26edce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
